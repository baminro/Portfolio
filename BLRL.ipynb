{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrRQ1xRIkYJVqtgKHZB068",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baminro/Portfolio/blob/main/BLRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoWER3_ZYLJS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "print(\"GPU:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"Num GPUs:\", len(physical_devices))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CJeECMLdYWq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import read_csv, set_option\n",
        "from pandas.plotting import scatter_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import datetime\n",
        "import math\n",
        "from numpy.random import choice\n",
        "import random\n",
        "\n",
        "from keras.layers import Input, Dense, Flatten, Dropout\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pylab as plt\n",
        "import matplotlib.dates as mdates # Formatting dates\n",
        "from sklearn.covariance import LedoitWolf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6rFk3GQ0YWtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Diable the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "ekjr2WlqYWwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "directory = r'/content/drive/MyDrive/Adjusted'\n",
        "df=[]\n",
        "files_in_directory = os.listdir(directory)\n",
        "filtered_files = [file for file in files_in_directory if file.endswith(\".csv\")]\n",
        "for file in filtered_files:\n",
        "    path_to_file = os.path.join(directory, file)\n",
        "    aux=pd.read_csv(path_to_file, encoding=\"utf-16\")\n",
        "    df.append(aux)\n",
        "df=pd.concat(df)\n",
        "df['<TICKER>']= df['<TICKER>'].str.replace('-ت', '')\n",
        "df['<COL12>']= df['<COL12>'].str.replace('-ت', '')\n",
        "df['<COL12>']= df['<COL12>'].str.replace('_', ' ')\n",
        "df['<TICKER>']= df['<TICKER>'].str.replace('_', ' ')\n",
        "\n",
        "\n",
        "df.to_excel('/content/drive/MyDrive/out.xlsx')\n",
        "import datetime\n",
        "from datetime import datetime as dt\n",
        "df['<DTYYYYMMDD>']=df['<DTYYYYMMDD>'].apply(lambda x : dt.strptime(str(x), '%Y%m%d'))\n",
        "\n",
        "\n",
        "#df['<DTYYYYMMDD>'] = pd.to_datetime(df['<DTYYYYMMDD>']).dt.date\n",
        "#df['<DTYYYYMMDD>'] = df['<DTYYYYMMDD>'].dt.strftime('%Y%m%d')\n",
        "df=df.set_index('<DTYYYYMMDD>')\n",
        "df"
      ],
      "metadata": {
        "id": "neeOufTOYWy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['SMAclose5']=df.groupby('<COL11>')['<CLOSE>'].transform(lambda x: x.rolling(5).mean())\n",
        "df['SMAclose30']=df.groupby('<COL11>')['<CLOSE>'].transform(lambda x: x.rolling(30).mean())\n",
        "df['EMAclose5']=df.groupby('<COL11>')['<CLOSE>'].transform(lambda x: x.ewm(span =5, min_periods = 5 - 1).mean())\n",
        "df['EMAclose30']=df.groupby('<COL11>')['<CLOSE>'].transform(lambda x: x.ewm(span =30, min_periods = 5 - 1).mean())\n",
        "MA = df['SMAclose5']\n",
        "SD = df.groupby('<COL11>')['<CLOSE>'].transform(lambda x: x.rolling(window=5).std().mean())\n",
        "#df['MiddleBandclose'] = MA\n",
        "df['UpperBandclose'] = MA + (2 * SD)\n",
        "df['LowerBandclose'] = MA - (2 * SD)\n",
        "\n",
        "df['FIclose'] =df.groupby('<COL11>')['<CLOSE>'].transform(lambda x: x.diff(1)) * df['<VOL>']\n",
        "\n",
        "df['dm'] = ((df['<HIGH>'] + df['<LOW>'])/2) - ((df.groupby('<COL11>')['<HIGH>'].transform(lambda x: x.shift(1)) + df.groupby('<COL11>')['<LOW>'].transform(lambda x: x.shift(1)))/2)\n",
        "df['br'] = (df['<VOL>'] / 100000000) / ((df['<HIGH>'] - df['<LOW>']))\n",
        "df['EMV'] = df['dm']/ df['br']\n",
        "df['EMV_MAclose'] =df.groupby('<COL11>')['EMV'].transform(lambda x: x.rolling(5).mean())\n",
        "del df['dm']\n",
        "del df['br']\n",
        "\n",
        "\n",
        "def rsi_transform(price_series, window=5):\n",
        "    delta = price_series.diff()\n",
        "    gain = delta.clip(lower=0)\n",
        "    loss = -delta.clip(upper=0)\n",
        "\n",
        "    # Use EMA smoothing (standard for RSI)\n",
        "    avg_gain = gain.ewm(alpha=1/window, adjust=False).mean()\n",
        "    avg_loss = loss.ewm(alpha=1/window, adjust=False).mean()\n",
        "\n",
        "    rs = avg_gain / avg_loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "# 3. Calculate RSI by ticker using transform\n",
        "df['RSIclose'] = (df.groupby('<COL11>')['<CLOSE>']\n",
        "                .transform(rsi_transform, window=5))\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "JLahoN_nYW1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Log-return of 'close' per stock\n",
        "df['log_returnC'] = df.groupby('<COL11>')['<CLOSE>'].transform(lambda x: np.log(x / x.shift(1)))\n",
        "df['log_returnO'] = df.groupby('<COL11>')['<OPEN>'].transform(lambda x: np.log(x / x.shift(1)))\n",
        "df['log_returnH'] = df.groupby('<COL11>')['<HIGH>'].transform(lambda x: np.log(x / x.shift(1)))\n",
        "df['log_returnL'] = df.groupby('<COL11>')['<LOW>'].transform(lambda x: np.log(x / x.shift(1)))\n",
        "# Log-change of 'volume' per stock (optional)\n",
        "df['log_volume_change'] = df.groupby('<COL11>')['<VOL>'].transform(lambda x: np.log(x / x.shift(1)))\n",
        "\n",
        "\n",
        "# Replace inf/-inf with NaN\n",
        "dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Option 1: Drop rows with NaNs\n",
        "dataset.dropna(inplace=True)\n",
        "\n",
        "# Option 2: Fill NaNs (if preserving rows)\n",
        "dataset.fillna(0, inplace=True)\n",
        "numeric_df = dataset.select_dtypes(include=[np.number])\n",
        "print(np.isinf(dataset).sum().sum())\n",
        "\n",
        "# Check for NaNs\n",
        "print(dataset.isna().sum().sum())\n",
        "\n",
        "# Check for very large values\n",
        "print((np.abs(dataset) > 1e308).sum().sum())\n",
        "\n",
        "# Fill missing values (first row in each group)\n",
        "df[['log_returnC','log_returnO','log_returnH','log_returnL', 'log_volume_change']] = df[['log_returnC','log_returnO','log_returnH','log_returnL', 'log_volume_change']]\n"
      ],
      "metadata": {
        "id": "9rT7kb2_YW4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.reset_index()\n",
        "df=df[['<DTYYYYMMDD>','<CLOSE>','<OPEN>',\t'<HIGH>',\t'<LOW>','<VOL>','<COL11>','<TICKER>','SMAclose5',\t'SMAclose30',\t'EMAclose5',\t'EMAclose30',\t'UpperBandclose',\t'LowerBandclose',\t'FIclose',\t'EMV_MAclose',\t'RSIclose']]\n",
        "df.columns = ['Date','<CLOSE>','<OPEN>','<HIGH>',\t'<LOW>','<VOL>','<COL11>','<TICKER>','SMAclose5',\t'SMAclose30',\t'EMAclose5',\t'EMAclose30',\t'UpperBandclose',\t'LowerBandclose',\t'FIclose',\t'EMV_MAclose', 'RSIclose']\n",
        "df=df[df['Date']<='2025-05-17']"
      ],
      "metadata": {
        "id": "M8M-jW5pYW7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.pivot(index='Date', columns='<COL11>', values=['<CLOSE>','<OPEN>',\t'<HIGH>',\t'<LOW>','<VOL>','SMAclose5',\t'UpperBandclose',\t'LowerBandclose',\t'FIclose',\t'EMV_MAclose',\t'RSIclose'])\n",
        "\n",
        "df=df.fillna(method='ffill')"
      ],
      "metadata": {
        "id": "gV9eayMZYW92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col = ('<CLOSE>', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "col = ('<OPEN>', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "col = ('<HIGH>', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "col = ('<LOW>', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "col = ('<VOL>', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"col = ('log_returnC', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "col = ('log_returnO', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "col = ('log_returnH', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "col = ('log_returnL', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "col = ('log_volume_change', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\"\"\"\n",
        "\n",
        "\n",
        "col = ('SMAclose5', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "'''col = ('SMAclose30', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "col = ('EMAclose5', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "col = ('EMAclose30', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]'''\n",
        "\n",
        "col = ('UpperBandclose', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "col = ('LowerBandclose', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "col = ('FIclose', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "col = ('EMV_MAclose', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "\n",
        "col = ('RSIclose', 'IDXS')\n",
        "new_order = [c for c in df.columns if c != col] + [col]\n",
        "df = df[new_order]\n",
        "df"
      ],
      "metadata": {
        "id": "qiGR8_R5YXAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# peek at data\n",
        "set_option('display.width', 100)\n",
        "dataset=dataset.reset_index()\n",
        "dataset['Date'] = pd.to_datetime(dataset['Date'],utc=True)\n",
        "dataset['Date'] = dataset['Date'].dt.strftime('%Y-%m-%d')\n",
        "dataset=dataset.set_index('Date')\n",
        "#del dataset['index']\n",
        "dataset.head(5)"
      ],
      "metadata": {
        "id": "KW0KSNVwYXDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataindex=dataset[[('<CLOSE>', 'IDXS')]]\n",
        "dataindex\n",
        "dataindex.to_csv('/content/drive/MyDrive/indexnew.csv')"
      ],
      "metadata": {
        "id": "eRSX_ionY79r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.to_csv('/content/drive/MyDrive/crypto_portfolionew.csv')\n"
      ],
      "metadata": {
        "id": "cmsNj2cRY8AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from IPython.core.debugger import set_trace\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import cvxpy as cp\n",
        "#define a function portfolio\n",
        "def portfolio(returns, weights):\n",
        "    weights = np.array(weights)\n",
        "    rets = returns.mean() * 252\n",
        "    retsmean=returns.mean()\n",
        "    rets12=returns-retsmean\n",
        "    lenx=rets12.shape[0]\n",
        "    rf=0.21/252\n",
        "    \"\"\"for i in range(lenx):\n",
        "            if (rets12.iloc[i]>0):\n",
        "                rets12.loc[:][i]=0\"\"\"\n",
        "\n",
        "    rets12[rets12 > 0] = 0\n",
        "    semicov=((rets12.T.dot(rets12))/(lenx-1))*252\n",
        "    #covs = returns.cov() * 252\n",
        "    P_ret = rets\n",
        "    P_vol =returns.std()* np.sqrt(252)\n",
        "    P_semivol =semicov* np.sqrt(252)\n",
        "    P_sharpe = (P_ret-rf*252) / P_vol\n",
        "    P_sortino = (P_ret-rf*252) / P_semivol\n",
        "    ## 1. Calmar Ratio\n",
        "    cumulative_returns = (returns + 1).cumprod()\n",
        "    running_max = cumulative_returns.cummax()\n",
        "    drawdown = (cumulative_returns - running_max) / running_max\n",
        "    max_drawdown = drawdown.min()\n",
        "\n",
        "    # daily → annualized compound return\n",
        "    annualized_return = (cumulative_returns.iloc[-1]) ** (252 / len(cumulative_returns)) - 1\n",
        "    calmar_ratio = (annualized_return-rf*252) / abs(max_drawdown) if max_drawdown != 0 else np.nan\n",
        "\n",
        "    ## 2. Excess Return to Value-at-Risk (VaR)\n",
        "    excess_return = returns.mean() - rf\n",
        "    var_95 = np.percentile(returns, 5)\n",
        "    excess_return_to_var = excess_return*252 / abs(var_95) if var_95 != 0 else np.nan\n",
        "\n",
        "    # 3. Excess Return to Conditional Value-at-Risk (CVaR)\n",
        "    excess_return = returns.mean() - rf\n",
        "    var_95 = np.percentile(returns, 5)\n",
        "    cvar_95 = returns[returns <= var_95].mean()\n",
        "    excess_return_to_cvar = excess_return*252 / abs(cvar_95) if cvar_95 != 0 else np.nan\n",
        "\n",
        "    return np.array([P_ret, P_vol,P_semivol, P_sharpe,P_sortino, calmar_ratio, excess_return_to_var,excess_return_to_cvar])\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"def get_positional_encoding(seq_len, d_model):\n",
        "    pos = np.arange(seq_len)[:, np.newaxis]           # shape: (seq_len, 1)\n",
        "    i = np.arange(d_model)[np.newaxis, :]            # shape: (1, d_model)\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    angle_rads = pos * angle_rates\n",
        "\n",
        "    # Apply sin to even indices, cos to odd indices\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]       # shape: (1, seq_len, d_model)\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\"\"\"\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Positional encoding helper\n",
        "# -------------------------\n",
        "\"\"\"def get_positional_encoding(seq_len, d_model):\n",
        "    pos = np.arange(seq_len)[:, np.newaxis]           # (seq_len, 1)\n",
        "    i = np.arange(d_model)[np.newaxis, :]             # (1, d_model)\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    angle_rads = pos * angle_rates\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]        # (1, seq_len, d_model)\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Subclassed transformer\n",
        "# -------------------------\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, n_assets, seq_len, d_model=64, num_heads=4, dff=128, num_layers=2, dropout=0.1, name=\"transformer_multioutput\"):\n",
        "        super().__init__(name=name)\n",
        "        self.seq_len = seq_len\n",
        "        self.d_model = d_model\n",
        "        self.n_assets = n_assets\n",
        "        self.pos_encoding = get_positional_encoding(seq_len, d_model)\n",
        "\n",
        "        # input projection\n",
        "        self.input_proj = layers.Dense(d_model, name=\"input_projection\")\n",
        "\n",
        "        # encoder layers (store layers in lists but give each layer explicit name)\n",
        "        self.attn_layers = []\n",
        "        self.norm1_layers = []\n",
        "        self.ffn_layers = []\n",
        "        self.norm2_layers = []\n",
        "        for i in range(num_layers):\n",
        "            attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, name=f\"multihead_attention_{i}\")\n",
        "            norm1 = layers.LayerNormalization(epsilon=1e-6, name=f\"attn_norm_{i}\")\n",
        "            ffn = tf.keras.Sequential([\n",
        "                layers.Dense(dff, activation=\"relu\", name=f\"ff_{i}_dense_1\"),\n",
        "                layers.Dense(d_model, name=f\"ff_{i}_dense_2\")\n",
        "            ], name=f\"ffn_{i}\")\n",
        "            norm2 = layers.LayerNormalization(epsilon=1e-6, name=f\"ffn_norm_{i}\")\n",
        "            self.attn_layers.append(attn)\n",
        "            self.norm1_layers.append(norm1)\n",
        "            self.ffn_layers.append(ffn)\n",
        "            self.norm2_layers.append(norm2)\n",
        "\n",
        "        # heads\n",
        "        self.mean_head = layers.Dense(n_assets, name=\"mean_head\")\n",
        "        self.var_head = layers.Dense(n_assets, activation=\"softplus\", name=\"var_head\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # inputs shape: (batch, seq_len, n_assets)\n",
        "        x = self.input_proj(inputs)                    # (batch, seq_len, d_model)\n",
        "        x = x + self.pos_encoding[:, :self.seq_len, :] # broadcasting positional encoding\n",
        "\n",
        "        for i, (attn, norm1, ffn, norm2) in enumerate(zip(self.attn_layers, self.norm1_layers, self.ffn_layers, self.norm2_layers)):\n",
        "            attn_out = attn(x, x)                      # self-attention\n",
        "            x = norm1(x + attn_out)                    # residual + norm\n",
        "            ffn_out = ffn(x)\n",
        "            x = norm2(x + ffn_out)                     # residual + norm\n",
        "\n",
        "        # use last time step representation for heads (can be changed if desired)\n",
        "        last = x[:, -1, :]                             # (batch, d_model)\n",
        "        mean_out = self.mean_head(last)\n",
        "        var_out = self.var_head(last)\n",
        "        return {\"mean_head\": mean_out, \"var_head\": var_out}\n",
        "\n",
        "    # keep compile signature convenient\n",
        "    def compile(self, optimizer='adam', loss=None, metrics=None, **kwargs):\n",
        "        if loss is None:\n",
        "            loss = {'mean_head': 'mse', 'var_head': 'mse'}\n",
        "        super().compile(optimizer=optimizer, loss=loss, metrics=metrics, **kwargs)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper (keeps same naming across train/eval)\n",
        "# -------------------------\n",
        "def build_transformer_for_env(env, n_assets=18, seq_len=60, d_model=64, num_heads=4, dff=128, num_layers=2, dropout=0.1, model_path=None):\n",
        "    model = TransformerModel(n_assets=n_assets, seq_len=seq_len, d_model=d_model, num_heads=num_heads, dff=dff, num_layers=num_layers, dropout=dropout)\n",
        "    # build with dummy input\n",
        "    dummy = tf.zeros((1, seq_len, n_assets))\n",
        "    _ = model(dummy)\n",
        "    # compile\n",
        "    model.compile(optimizer='adam', loss={'mean_head': 'mse', 'var_head': 'mse'}, metrics={'mean_head':'mae','var_head':'mae'})\n",
        "    env.transformer_model = model\n",
        "\n",
        "    # Optionally load weights safely\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded transformer weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load transformer weights from {model_path}: {e}\\n→ Continuing with freshly initialized transformer.\")\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Positional encoding helper\n",
        "# -------------------------\n",
        "\"\"\"def get_positional_encoding(seq_len, d_model):\n",
        "    pos = np.arange(seq_len)[:, np.newaxis]           # (seq_len, 1)\n",
        "    i = np.arange(d_model)[np.newaxis, :]             # (1, d_model)\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    angle_rads = pos * angle_rates\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]        # (1, seq_len, d_model)\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Simple Graph Convolution Layer\n",
        "# -------------------------\n",
        "class GraphConvolution(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_dim, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.output_dim = output_dim\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_dim, self.output_dim),\n",
        "            initializer=\"glorot_uniform\",\n",
        "            trainable=True,\n",
        "            name=\"gcn_weight\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, adjacency_matrix):\n",
        "        # inputs: [batch, n_assets, features]\n",
        "        support = tf.matmul(inputs, self.w)                         # (batch, n_assets, output_dim)\n",
        "        output = tf.matmul(adjacency_matrix, support)               # (batch, n_assets, output_dim)\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# GCN + Transformer hybrid model\n",
        "# -------------------------\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, n_assets, seq_len, node_features=1,\n",
        "                 d_model=64, num_heads=4, dff=128, num_layers=2,\n",
        "                 gcn_units=32, dropout=0.1, name=\"gcn_transformer\"):\n",
        "        super().__init__(name=name)\n",
        "        self.seq_len = seq_len\n",
        "        self.d_model = d_model\n",
        "        self.n_assets = n_assets\n",
        "        self.node_features = node_features\n",
        "        self.pos_encoding = get_positional_encoding(seq_len, d_model)\n",
        "\n",
        "        # --- Graph convolution ---\n",
        "        self.gcn1 = GraphConvolution(gcn_units, activation=\"relu\", name=\"gcn1\")\n",
        "        self.gcn2 = GraphConvolution(gcn_units, activation=\"relu\", name=\"gcn2\")\n",
        "\n",
        "        # projection before transformer\n",
        "        self.input_proj = layers.Dense(d_model, name=\"input_projection\")\n",
        "\n",
        "        # --- Transformer encoder stack ---\n",
        "        self.attn_layers, self.norm1_layers, self.ffn_layers, self.norm2_layers = [], [], [], []\n",
        "        for i in range(num_layers):\n",
        "            attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, name=f\"multihead_attention_{i}\")\n",
        "            norm1 = layers.LayerNormalization(epsilon=1e-6, name=f\"attn_norm_{i}\")\n",
        "            ffn = tf.keras.Sequential([\n",
        "                layers.Dense(dff, activation=\"relu\", name=f\"ff_{i}_dense_1\"),\n",
        "                layers.Dense(d_model, name=f\"ff_{i}_dense_2\")\n",
        "            ], name=f\"ffn_{i}\")\n",
        "            norm2 = layers.LayerNormalization(epsilon=1e-6, name=f\"ffn_norm_{i}\")\n",
        "            self.attn_layers.append(attn)\n",
        "            self.norm1_layers.append(norm1)\n",
        "            self.ffn_layers.append(ffn)\n",
        "            self.norm2_layers.append(norm2)\n",
        "\n",
        "        # --- Prediction heads ---\n",
        "        self.mean_head = layers.Dense(n_assets, name=\"mean_head\")\n",
        "        self.var_head = layers.Dense(n_assets, activation=\"softplus\", name=\"var_head\")\n",
        "\n",
        "    def call(self, inputs, adjacency_matrix=None, training=False):\n",
        "        # If input is 3D [batch, seq_len, n_assets], expand to 4D\n",
        "        if len(inputs.shape) == 3:\n",
        "            inputs = tf.expand_dims(inputs, -1)  # -> [batch, seq_len, n_assets, 1]\n",
        "\n",
        "        if adjacency_matrix is None:\n",
        "            adjacency_matrix = tf.eye(self.n_assets)\n",
        "\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        x_gcn = []\n",
        "\n",
        "        # apply GCN at each time step\n",
        "        for t in range(self.seq_len):\n",
        "            x_t = inputs[:, t, :, :]                  # [batch, n_assets, node_features]\n",
        "            x_t = self.gcn1(x_t, adjacency_matrix)\n",
        "            x_t = self.gcn2(x_t, adjacency_matrix)\n",
        "            x_gcn.append(x_t)\n",
        "\n",
        "        # stack time dimension\n",
        "        x = tf.stack(x_gcn, axis=1)                   # [batch, seq_len, n_assets, gcn_units]\n",
        "\n",
        "        # flatten assets for transformer\n",
        "        x = tf.reshape(x, [batch_size, self.seq_len, self.n_assets * x.shape[-1]])\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        # add positional encoding\n",
        "        x += self.pos_encoding[:, :self.seq_len, :]\n",
        "\n",
        "        # transformer encoder\n",
        "        for attn, norm1, ffn, norm2 in zip(self.attn_layers, self.norm1_layers, self.ffn_layers, self.norm2_layers):\n",
        "            attn_out = attn(x, x)\n",
        "            x = norm1(x + attn_out)\n",
        "            ffn_out = ffn(x)\n",
        "            x = norm2(x + ffn_out)\n",
        "\n",
        "        # final step representation\n",
        "        last = x[:, -1, :]                            # [batch, d_model]\n",
        "        mean_out = self.mean_head(last)\n",
        "        var_out = self.var_head(last)\n",
        "        return {\"mean_head\": mean_out, \"var_head\": var_out}\n",
        "\n",
        "\n",
        "    def compile(self, optimizer=\"adam\", loss=None, metrics=None, **kwargs):\n",
        "        if loss is None:\n",
        "            loss = {\"mean_head\": \"mse\", \"var_head\": \"mse\"}\n",
        "        super().compile(optimizer=optimizer, loss=loss, metrics=metrics, **kwargs)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper\n",
        "# -------------------------\n",
        "def build_transformer_for_env(env, n_assets=18, seq_len=60, node_features=1,\n",
        "                                  d_model=64, num_heads=4, dff=128, num_layers=2,\n",
        "                                  gcn_units=32, dropout=0.1, model_path=None):\n",
        "    model = TransformerModel(n_assets=n_assets, seq_len=seq_len, node_features=node_features,\n",
        "                           d_model=d_model, num_heads=num_heads, dff=dff,\n",
        "                           num_layers=num_layers, gcn_units=gcn_units, dropout=dropout)\n",
        "    dummy_x = tf.zeros((1, seq_len, n_assets, node_features))\n",
        "    _ = model(dummy_x)  # build\n",
        "    model.compile(optimizer=\"adam\", loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "                  metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"})\n",
        "    env.transformer_model = model\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded GCN+Transformer weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load weights from {model_path}: {e}\")\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Simple Graph Convolution Layer (same semantics as before)\n",
        "# -------------------------\n",
        "\"\"\"class GraphConvolution(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_dim, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.output_dim = output_dim\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = int(input_shape[-1])\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_dim, self.output_dim),\n",
        "            initializer=\"glorot_uniform\",\n",
        "            trainable=True,\n",
        "            name=\"gcn_weight\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, adjacency_matrix):\n",
        "        # inputs: [batch, n_assets, features]\n",
        "        support = tf.linalg.matmul(inputs, self.w)            # (batch, n_assets, output_dim)\n",
        "        output = tf.linalg.matmul(adjacency_matrix, support)  # (batch, n_assets, output_dim)\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Small residual TCN block (1D dilated conv)\n",
        "# -------------------------\n",
        "class TCNBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size=3, dilation_rate=1, dropout=0.0, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.conv = layers.Conv1D(filters, kernel_size, padding=\"causal\", dilation_rate=dilation_rate)\n",
        "        self.bn = layers.BatchNormalization()\n",
        "        self.act = layers.Activation(\"relu\")\n",
        "        self.dropout = layers.Dropout(dropout) if dropout and dropout > 0 else None\n",
        "        self.res_conv = None  # set later if projection needed\n",
        "        self.filters = filters\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        in_filters = int(input_shape[-1])\n",
        "        if in_filters != self.filters:\n",
        "            self.res_conv = layers.Conv1D(self.filters, kernel_size=1, padding=\"same\")\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        y = self.conv(x)\n",
        "        y = self.bn(y, training=training)\n",
        "        y = self.act(y)\n",
        "        if self.dropout:\n",
        "            y = self.dropout(y, training=training)\n",
        "        if self.res_conv is not None:\n",
        "            x = self.res_conv(x)\n",
        "        return layers.Add()([x, y])\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# GCN -> LSTM -> TCN hybrid model\n",
        "# -------------------------\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, n_assets, seq_len, node_features=1,\n",
        "                 gcn_units=[32, 32], lstm_units=64,\n",
        "                 tcn_filters=64, tcn_kernel=3, tcn_blocks=2,\n",
        "                 **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_assets = n_assets\n",
        "        self.seq_len = seq_len\n",
        "        self.node_features = node_features\n",
        "\n",
        "        # GCN layers (corrected)\n",
        "        self.gcn_layers = [GraphConvolution(units, activation=\"relu\") for units in gcn_units]\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = layers.LSTM(lstm_units, return_sequences=True)\n",
        "\n",
        "        # TCN (stacked Conv1D blocks)\n",
        "        self.tcn_blocks = []\n",
        "        for i in range(tcn_blocks):\n",
        "            self.tcn_blocks.append(\n",
        "                layers.Conv1D(filters=tcn_filters, kernel_size=tcn_kernel,\n",
        "                              dilation_rate=2**i, padding=\"causal\", activation=\"relu\")\n",
        "            )\n",
        "\n",
        "        # Prediction heads\n",
        "        self.mean_head = layers.Dense(n_assets, name=\"mean_head\")\n",
        "        self.var_head = layers.Dense(n_assets, activation=\"softplus\", name=\"var_head\")\n",
        "\n",
        "    def call(self, inputs, adjacency_matrix=None, training=False):\n",
        "        # Accept 3D or 4D input\n",
        "        if inputs.shape.rank == 3:\n",
        "            inputs = tf.expand_dims(inputs, -1)  # (batch, seq, n_assets, 1)\n",
        "\n",
        "        if adjacency_matrix is None:\n",
        "            adjacency_matrix = tf.eye(self.n_assets)\n",
        "\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        gcn_out = []\n",
        "\n",
        "        # Apply GCN at each time step\n",
        "        for t in range(self.seq_len):\n",
        "            x_t = inputs[:, t, :, :]  # (batch, n_assets, features)\n",
        "            for gcn in self.gcn_layers:\n",
        "                x_t = gcn(x_t, adjacency_matrix)\n",
        "            gcn_out.append(x_t)\n",
        "\n",
        "        # Stack over time\n",
        "        x = tf.stack(gcn_out, axis=1)  # (batch, seq, n_assets, gcn_units)\n",
        "        x = tf.reshape(x, [batch_size, self.seq_len, -1])  # flatten assets\n",
        "\n",
        "        # LSTM\n",
        "        x = self.lstm(x)\n",
        "\n",
        "        # TCN\n",
        "        for conv in self.tcn_blocks:\n",
        "            x = conv(x)\n",
        "\n",
        "        # Use last time step\n",
        "        last = x[:, -1, :]\n",
        "        mean_out = self.mean_head(last)\n",
        "        var_out = self.var_head(last)\n",
        "        return {\"mean_head\": mean_out, \"var_head\": var_out}\n",
        "\n",
        "\n",
        "    def compile(self, optimizer=\"adam\", loss=None, metrics=None, **kwargs):\n",
        "        if loss is None:\n",
        "            loss = {\"mean_head\": \"mse\", \"var_head\": \"mse\"}\n",
        "        super().compile(optimizer=optimizer, loss=loss, metrics=metrics, **kwargs)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper for env (keeps naming, weight loading)\n",
        "# -------------------------\n",
        "def build_transformer_for_env(env,\n",
        "                              n_assets=18,\n",
        "                              seq_len=60,\n",
        "                              node_features=1,\n",
        "                              gcn_units=[32, 32],\n",
        "                              lstm_units=64,\n",
        "                              tcn_filters=64,\n",
        "                              tcn_kernel=3,\n",
        "                              tcn_blocks=2,\n",
        "                              model_path=None):\n",
        "    model = TransformerModel(\n",
        "        n_assets=n_assets,\n",
        "        seq_len=seq_len,\n",
        "        node_features=node_features,\n",
        "        gcn_units=gcn_units,\n",
        "        lstm_units=lstm_units,\n",
        "        tcn_filters=tcn_filters,\n",
        "        tcn_kernel=tcn_kernel,\n",
        "        tcn_blocks=tcn_blocks\n",
        "    )\n",
        "\n",
        "    # Build with dummy input\n",
        "    dummy_x = tf.zeros((1, seq_len, n_assets, node_features))\n",
        "    _ = model(dummy_x)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "        metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"}\n",
        "    )\n",
        "    env.transformer_model = model\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded GCN+LSTM+TCN weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load weights from {model_path}: {e}\")\"\"\"\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Simple Graph Convolution Layer\n",
        "# -------------------------\n",
        "\"\"\"class GraphConvolution(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_dim, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.output_dim = output_dim\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = int(input_shape[-1])\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_dim, self.output_dim),\n",
        "            initializer=\"glorot_uniform\",\n",
        "            trainable=True,\n",
        "            name=\"gcn_weight\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, adjacency_matrix):\n",
        "        # inputs: [batch, n_assets, features]\n",
        "        support = tf.linalg.matmul(inputs, self.w)            # (batch, n_assets, output_dim)\n",
        "        output = tf.linalg.matmul(adjacency_matrix, support)  # (batch, n_assets, output_dim)\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Pure GNN Model\n",
        "# -------------------------\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, n_assets, seq_len, node_features=1,\n",
        "                 gcn_units=[32, 32], **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_assets = n_assets\n",
        "        self.seq_len = seq_len\n",
        "        self.node_features = node_features\n",
        "\n",
        "        # Stack of GCN layers\n",
        "        self.gcn_layers = [GraphConvolution(units, activation=\"relu\") for units in gcn_units]\n",
        "\n",
        "        # Prediction heads\n",
        "        self.mean_head = layers.Dense(n_assets, name=\"mean_head\")\n",
        "        self.var_head = layers.Dense(n_assets, activation=\"softplus\", name=\"var_head\")\n",
        "\n",
        "    def call(self, inputs, adjacency_matrix=None, training=False):\n",
        "        # Accept 3D or 4D input\n",
        "        if inputs.shape.rank == 3:\n",
        "            inputs = tf.expand_dims(inputs, -1)  # (batch, seq, n_assets, 1)\n",
        "\n",
        "        if adjacency_matrix is None:\n",
        "            adjacency_matrix = tf.eye(self.n_assets)\n",
        "\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        gcn_out = []\n",
        "\n",
        "        # Apply GCN at each time step\n",
        "        for t in range(self.seq_len):\n",
        "            x_t = inputs[:, t, :, :]  # (batch, n_assets, features)\n",
        "            for gcn in self.gcn_layers:\n",
        "                x_t = gcn(x_t, adjacency_matrix)\n",
        "            gcn_out.append(x_t)\n",
        "\n",
        "        # Stack across time\n",
        "        x = tf.stack(gcn_out, axis=1)  # (batch, seq, n_assets, gcn_units)\n",
        "\n",
        "        # Flatten assets + time for final dense layer\n",
        "        x = tf.reshape(x, [batch_size, self.seq_len * self.n_assets * x.shape[-1]])\n",
        "\n",
        "        # Prediction heads\n",
        "        mean_out = self.mean_head(x)\n",
        "        var_out = self.var_head(x)\n",
        "        return {\"mean_head\": mean_out, \"var_head\": var_out}\n",
        "\n",
        "    def compile(self, optimizer=\"adam\", loss=None, metrics=None, **kwargs):\n",
        "        if loss is None:\n",
        "            loss = {\"mean_head\": \"mse\", \"var_head\": \"mse\"}\n",
        "        super().compile(optimizer=optimizer, loss=loss, metrics=metrics, **kwargs)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper for env\n",
        "# -------------------------\n",
        "def build_transformer_for_env(env,\n",
        "                      n_assets=18,\n",
        "                      seq_len=60,\n",
        "                      node_features=1,\n",
        "                      gcn_units=[32, 32],\n",
        "                      model_path=None):\n",
        "    model = TransformerModel(\n",
        "        n_assets=n_assets,\n",
        "        seq_len=seq_len,\n",
        "        node_features=node_features,\n",
        "        gcn_units=gcn_units\n",
        "    )\n",
        "\n",
        "    # Build with dummy input\n",
        "    dummy_x = tf.zeros((1, seq_len, n_assets, node_features))\n",
        "    _ = model(dummy_x)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "        metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"}\n",
        "    )\n",
        "    env.transformer_model = model\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded GNN weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load weights (architecture mismatch). Starting fresh.\")\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# TCN residual block\n",
        "# -------------------------\n",
        "\"\"\"class TCNBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size=3, dilation_rate=1, dropout=0.0, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.conv = layers.Conv1D(filters, kernel_size,\n",
        "                                  padding=\"causal\",\n",
        "                                  dilation_rate=dilation_rate)\n",
        "        self.bn = layers.BatchNormalization()\n",
        "        self.act = layers.Activation(\"relu\")\n",
        "        self.dropout = layers.Dropout(dropout) if dropout and dropout > 0 else None\n",
        "        self.res_conv = None\n",
        "        self.filters = filters\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        in_filters = int(input_shape[-1])\n",
        "        if in_filters != self.filters:\n",
        "            self.res_conv = layers.Conv1D(self.filters, kernel_size=1, padding=\"same\")\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        y = self.conv(x)\n",
        "        y = self.bn(y, training=training)\n",
        "        y = self.act(y)\n",
        "        if self.dropout:\n",
        "            y = self.dropout(y, training=training)\n",
        "        if self.res_conv is not None:\n",
        "            x = self.res_conv(x)\n",
        "        return layers.Add()([x, y])\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Pure TCN Model\n",
        "# -------------------------\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, n_assets, seq_len,\n",
        "                 tcn_filters=64, tcn_kernel=3, tcn_blocks=3,\n",
        "                 dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_assets = n_assets\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # TCN stack\n",
        "        self.tcn_blocks = []\n",
        "        for i in range(tcn_blocks):\n",
        "            self.tcn_blocks.append(\n",
        "                TCNBlock(filters=tcn_filters,\n",
        "                         kernel_size=tcn_kernel,\n",
        "                         dilation_rate=2**i,\n",
        "                         dropout=dropout)\n",
        "            )\n",
        "\n",
        "        # Prediction heads\n",
        "        self.mean_head = layers.Dense(n_assets, name=\"mean_head\")\n",
        "        self.var_head = layers.Dense(n_assets, activation=\"softplus\", name=\"var_head\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Accept (batch, seq, n_assets) or (batch, seq, n_assets, 1)\n",
        "        if inputs.shape.rank == 4:\n",
        "            inputs = tf.squeeze(inputs, -1)  # (batch, seq, n_assets)\n",
        "\n",
        "        # Flatten assets → treat them as features per timestep\n",
        "        x = tf.reshape(inputs, [tf.shape(inputs)[0], self.seq_len, -1])  # (batch, seq, n_assets)\n",
        "\n",
        "        # Pass through stacked TCN blocks\n",
        "        for block in self.tcn_blocks:\n",
        "            x = block(x, training=training)\n",
        "\n",
        "        # Use last time step\n",
        "        last = x[:, -1, :]\n",
        "        mean_out = self.mean_head(last)\n",
        "        var_out = self.var_head(last)\n",
        "        return {\"mean_head\": mean_out, \"var_head\": var_out}\n",
        "\n",
        "    def compile(self, optimizer=\"adam\", loss=None, metrics=None, **kwargs):\n",
        "        if loss is None:\n",
        "            loss = {\"mean_head\": \"mse\", \"var_head\": \"mse\"}\n",
        "        super().compile(optimizer=optimizer, loss=loss, metrics=metrics, **kwargs)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper for env\n",
        "# -------------------------\n",
        "def build_transformer_for_env(env,\n",
        "                              n_assets=18,\n",
        "                              seq_len=60,\n",
        "                              tcn_filters=64,\n",
        "                              tcn_kernel=3,\n",
        "                              tcn_blocks=3,\n",
        "                              dropout=0.1,\n",
        "                              model_path=None):\n",
        "    model = TransformerModel(\n",
        "        n_assets=n_assets,\n",
        "        seq_len=seq_len,\n",
        "        tcn_filters=tcn_filters,\n",
        "        tcn_kernel=tcn_kernel,\n",
        "        tcn_blocks=tcn_blocks,\n",
        "        dropout=dropout\n",
        "    )\n",
        "\n",
        "    # Build with dummy input\n",
        "    dummy_x = tf.zeros((1, seq_len, n_assets))\n",
        "    _ = model(dummy_x)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "        metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"}\n",
        "    )\n",
        "    env.transformer_model = model\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded TCN weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load weights (architecture mismatch). Starting fresh.\")\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# TCN residual block\n",
        "# -------------------------\n",
        "class TCNBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size=3, dilation_rate=1, dropout=0.0, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.conv = layers.Conv1D(filters, kernel_size,\n",
        "                                  padding=\"causal\",\n",
        "                                  dilation_rate=dilation_rate)\n",
        "        self.bn = layers.BatchNormalization()\n",
        "        self.act = layers.Activation(\"relu\")\n",
        "        self.dropout = layers.Dropout(dropout) if dropout and dropout > 0 else None\n",
        "        self.res_conv = None\n",
        "        self.filters = filters\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        in_filters = int(input_shape[-1])\n",
        "        if in_filters != self.filters:\n",
        "            self.res_conv = layers.Conv1D(self.filters, kernel_size=1, padding=\"same\")\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        y = self.conv(x)\n",
        "        y = self.bn(y, training=training)\n",
        "        y = self.act(y)\n",
        "        if self.dropout:\n",
        "            y = self.dropout(y, training=training)\n",
        "        if self.res_conv is not None:\n",
        "            x = self.res_conv(x)\n",
        "        return layers.Add()([x, y])\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# TCN + LSTM Model\n",
        "# -------------------------\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, n_assets, seq_len,\n",
        "                 tcn_filters=64, tcn_kernel=3, tcn_blocks=3,\n",
        "                 lstm_units=64, dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_assets = n_assets\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # TCN stack\n",
        "        self.tcn_blocks = []\n",
        "        for i in range(tcn_blocks):\n",
        "            self.tcn_blocks.append(\n",
        "                TCNBlock(filters=tcn_filters,\n",
        "                         kernel_size=tcn_kernel,\n",
        "                         dilation_rate=2**i,\n",
        "                         dropout=dropout)\n",
        "            )\n",
        "\n",
        "        # LSTM after TCN\n",
        "        self.lstm = layers.LSTM(lstm_units, return_sequences=False)\n",
        "\n",
        "        # Prediction heads\n",
        "        self.mean_head = layers.Dense(n_assets, name=\"mean_head\")\n",
        "        self.var_head = layers.Dense(n_assets, activation=\"softplus\", name=\"var_head\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Accept (batch, seq, n_assets) or (batch, seq, n_assets, 1)\n",
        "        if inputs.shape.rank == 4:\n",
        "            inputs = tf.squeeze(inputs, -1)  # (batch, seq, n_assets)\n",
        "\n",
        "        # Flatten assets → treat them as features per timestep\n",
        "        x = tf.reshape(inputs, [tf.shape(inputs)[0], self.seq_len, -1])  # (batch, seq, features)\n",
        "\n",
        "        # Pass through TCN stack\n",
        "        for block in self.tcn_blocks:\n",
        "            x = block(x, training=training)\n",
        "\n",
        "        # LSTM layer (captures sequential dependencies after TCN)\n",
        "        x = self.lstm(x)\n",
        "\n",
        "        # Predictions\n",
        "        mean_out = self.mean_head(x)\n",
        "        var_out = self.var_head(x)\n",
        "        return {\"mean_head\": mean_out, \"var_head\": var_out}\n",
        "\n",
        "    def compile(self, optimizer=\"adam\", loss=None, metrics=None, **kwargs):\n",
        "        if loss is None:\n",
        "            loss = {\"mean_head\": \"mse\", \"var_head\": \"mse\"}\n",
        "        super().compile(optimizer=optimizer, loss=loss, metrics=metrics, **kwargs)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper for env\n",
        "# -------------------------\n",
        "def build_transformer_for_env(env,\n",
        "                              n_assets=18,\n",
        "                              seq_len=60,\n",
        "                              tcn_filters=64,\n",
        "                              tcn_kernel=3,\n",
        "                              tcn_blocks=3,\n",
        "                              lstm_units=64,\n",
        "                              dropout=0.1,\n",
        "                              model_path=None):\n",
        "    model = TransformerModel(\n",
        "        n_assets=n_assets,\n",
        "        seq_len=seq_len,\n",
        "        tcn_filters=tcn_filters,\n",
        "        tcn_kernel=tcn_kernel,\n",
        "        tcn_blocks=tcn_blocks,\n",
        "        lstm_units=lstm_units,\n",
        "        dropout=dropout\n",
        "    )\n",
        "\n",
        "    # Build with dummy input\n",
        "    dummy_x = tf.zeros((1, seq_len, n_assets))\n",
        "    _ = model(dummy_x)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "        metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"}\n",
        "    )\n",
        "    env.transformer_model = model\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded TCN+LSTM weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load weights (architecture mismatch). Starting fresh.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Positional Encoding\n",
        "# -------------------------\n",
        "\"\"\"def get_positional_encoding(seq_len, d_model):\n",
        "    pos = np.arange(seq_len)[:, np.newaxis]           # (seq_len, 1)\n",
        "    i = np.arange(d_model)[np.newaxis, :]             # (1, d_model)\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    angle_rads = pos * angle_rates\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]        # (1, seq_len, d_model)\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "# -------------------------\n",
        "# ProbSparse Attention\n",
        "# -------------------------\n",
        "class ProbSparseAttention(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        attn_out = self.mha(x, x)\n",
        "        attn_out = self.dropout(attn_out, training=training)\n",
        "        return self.norm(x + attn_out)\n",
        "\n",
        "# -------------------------\n",
        "# Distilling Layer (Conv1D)\n",
        "# -------------------------\n",
        "class DistillLayer(layers.Layer):\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.conv = layers.Conv1D(filters=d_model, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.conv(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return self.norm(x)\n",
        "\n",
        "# -------------------------\n",
        "# Informer Encoder Layer\n",
        "# -------------------------\n",
        "class InformerEncoderLayer(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = ProbSparseAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(dff, activation=\"relu\"),\n",
        "            layers.Dense(d_model)\n",
        "        ])\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.attn(x, training=training)\n",
        "        ffn_out = self.ffn(x)\n",
        "        ffn_out = self.dropout(ffn_out, training=training)\n",
        "        return self.norm(x + ffn_out)\n",
        "\n",
        "# -------------------------\n",
        "# Full Informer Model\n",
        "# -------------------------\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, n_assets, seq_len,\n",
        "                 d_model=64, num_heads=4, dff=128, num_layers=2,\n",
        "                 dropout=0.1, name=\"informer_multioutput\"):\n",
        "        super().__init__(name=name)\n",
        "        self.n_assets = n_assets\n",
        "        self.seq_len = seq_len\n",
        "        self.pos_encoding = get_positional_encoding(seq_len, d_model)\n",
        "\n",
        "        # input projection\n",
        "        self.input_proj = layers.Dense(d_model, name=\"input_projection\")\n",
        "\n",
        "        # encoder with distilling\n",
        "        self.enc_layers = []\n",
        "        for i in range(num_layers):\n",
        "            self.enc_layers.append(InformerEncoderLayer(d_model, num_heads, dff, dropout))\n",
        "            if i < num_layers - 1:\n",
        "                self.enc_layers.append(DistillLayer(d_model, dropout))\n",
        "\n",
        "        # heads\n",
        "        self.mean_head = layers.Dense(n_assets, name=\"mean_head\")\n",
        "        self.var_head = layers.Dense(n_assets, activation=\"softplus\", name=\"var_head\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.input_proj(inputs)\n",
        "        x += self.pos_encoding[:, :self.seq_len, :]\n",
        "        for layer in self.enc_layers:\n",
        "            x = layer(x, training=training)\n",
        "        last = x[:, -1, :]\n",
        "        mean_out = self.mean_head(last)\n",
        "        var_out = self.var_head(last)\n",
        "        return {\"mean_head\": mean_out, \"var_head\": var_out}\n",
        "\n",
        "    def compile(self, optimizer=\"adam\", loss=None, metrics=None, **kwargs):\n",
        "        if loss is None:\n",
        "            loss = {\"mean_head\": \"mse\", \"var_head\": \"mse\"}\n",
        "        super().compile(optimizer=optimizer, loss=loss, metrics=metrics, **kwargs)\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper for env\n",
        "# -------------------------\n",
        "def build_transformer_for_env(env,\n",
        "                              n_assets=18,\n",
        "                              seq_len=60,\n",
        "                              d_model=64,\n",
        "                              num_heads=4,\n",
        "                              dff=128,\n",
        "                              num_layers=2,\n",
        "                              dropout=0.1,\n",
        "                              model_path=None):\n",
        "    model = TransformerModel(\n",
        "        n_assets=n_assets,\n",
        "        seq_len=seq_len,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dff=dff,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout\n",
        "    )\n",
        "\n",
        "    dummy = tf.zeros((1, seq_len, n_assets))\n",
        "    _ = model(dummy)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "        metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"}\n",
        "    )\n",
        "    env.transformer_model = model\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded Informer weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load Informer weights from {model_path}: {e}\\n→ Starting fresh.\")\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Positional Encoding\n",
        "# -------------------------\n",
        "\"\"\"def get_positional_encoding(seq_len, d_model):\n",
        "    pos = np.arange(seq_len)[:, np.newaxis]          # (seq_len, 1)\n",
        "    i = np.arange(d_model)[np.newaxis, :]            # (1, d_model)\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    angle_rads = pos * angle_rates\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]       # (1, seq_len, d_model)\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "# -------------------------\n",
        "# Simple Informer-like model\n",
        "# -------------------------\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, n_assets, seq_len, d_model=64, num_heads=4, dff=128, num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.d_model = d_model\n",
        "        self.n_assets = n_assets\n",
        "\n",
        "        # positional encoding\n",
        "        self.pos_encoding = get_positional_encoding(seq_len, d_model)\n",
        "\n",
        "        # input projection\n",
        "        self.input_proj = layers.Dense(d_model, name=\"input_projection\")\n",
        "\n",
        "        # encoder stack\n",
        "        self.attn_layers = []\n",
        "        self.norm1_layers = []\n",
        "        self.ffn_layers = []\n",
        "        self.norm2_layers = []\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, name=f\"multihead_attention_{i}\")\n",
        "            norm1 = layers.LayerNormalization(epsilon=1e-6, name=f\"attn_norm_{i}\")\n",
        "            ffn = tf.keras.Sequential([\n",
        "                layers.Dense(dff, activation=\"relu\", name=f\"ff_{i}_dense_1\"),\n",
        "                layers.Dense(d_model, name=f\"ff_{i}_dense_2\")\n",
        "            ])\n",
        "            norm2 = layers.LayerNormalization(epsilon=1e-6, name=f\"ffn_norm_{i}\")\n",
        "            self.attn_layers.append(attn)\n",
        "            self.norm1_layers.append(norm1)\n",
        "            self.ffn_layers.append(ffn)\n",
        "            self.norm2_layers.append(norm2)\n",
        "\n",
        "        # prediction heads\n",
        "        self.mean_head = layers.Dense(n_assets, name=\"mean_head\")\n",
        "        self.var_head = layers.Dense(n_assets, activation=\"softplus\", name=\"var_head\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # inputs shape: (batch, seq_len, n_assets)\n",
        "        x = self.input_proj(inputs)\n",
        "        x += self.pos_encoding[:, :self.seq_len, :]  # add positional encoding\n",
        "\n",
        "        # encoder\n",
        "        for attn, norm1, ffn, norm2 in zip(self.attn_layers, self.norm1_layers, self.ffn_layers, self.norm2_layers):\n",
        "            attn_out = attn(x, x)\n",
        "            x = norm1(x + attn_out)\n",
        "            ffn_out = ffn(x)\n",
        "            x = norm2(x + ffn_out)\n",
        "\n",
        "        # use last timestep\n",
        "        last = x[:, -1, :]\n",
        "        mean_out = self.mean_head(last)\n",
        "        var_out = self.var_head(last)\n",
        "        return {\"mean_head\": mean_out, \"var_head\": var_out}\n",
        "\n",
        "    def compile(self, optimizer=\"adam\", loss=None, metrics=None, **kwargs):\n",
        "        if loss is None:\n",
        "            loss = {\"mean_head\": \"mse\", \"var_head\": \"mse\"}\n",
        "        super().compile(optimizer=optimizer, loss=loss, metrics=metrics, **kwargs)\n",
        "\n",
        "# -------------------------\n",
        "# Builder for environment\n",
        "# -------------------------\n",
        "def build_transformer_for_env(env, n_assets=18, seq_len=60, d_model=64, num_heads=4, dff=128, num_layers=2, dropout=0.1, model_path=None):\n",
        "    model = TransformerModel(\n",
        "        n_assets=n_assets,\n",
        "        seq_len=seq_len,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dff=dff,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout\n",
        "    )\n",
        "\n",
        "    # dummy input to build model\n",
        "    dummy_x = tf.zeros((1, seq_len, n_assets))\n",
        "    _ = model(dummy_x)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "        metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"}\n",
        "    )\n",
        "    env.transformer_model = model\n",
        "\n",
        "    # optionally load weights\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded Informer weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load weights. Starting fresh: {e}\")\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Positional Encoding\n",
        "# -------------------------\n",
        "\"\"\"def get_positional_encoding(seq_len, d_model):\n",
        "    pos = np.arange(seq_len)[:, np.newaxis]           # (seq_len, 1)\n",
        "    i = np.arange(d_model)[np.newaxis, :]             # (1, d_model)\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    angle_rads = pos * angle_rates\n",
        "    # apply sin to even indices, cos to odd indices\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]       # (1, seq_len, d_model)\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "# -------------------------\n",
        "# Standard Multi-Head Attention Layer\n",
        "# -------------------------\n",
        "class FullAttention(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        attn_out = self.mha(x, x)\n",
        "        attn_out = self.dropout(attn_out, training=training)\n",
        "        return self.norm(x + attn_out)  # residual + norm\n",
        "\n",
        "# -------------------------\n",
        "# Informer Encoder Layer\n",
        "# -------------------------\n",
        "class InformerEncoderLayer(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = FullAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(dff, activation=\"relu\"),\n",
        "            layers.Dense(d_model)\n",
        "        ])\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.attn(x, training=training)\n",
        "        ffn_out = self.ffn(x)\n",
        "        ffn_out = self.dropout(ffn_out, training=training)\n",
        "        return self.norm(x + ffn_out)\n",
        "\n",
        "# -------------------------\n",
        "# Short-Sequence Informer with Positional Encoding\n",
        "# -------------------------\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, n_assets, seq_len,\n",
        "                 d_model=64, num_heads=4, dff=128, num_layers=2,\n",
        "                 dropout=0.1, name=\"informer_short_pe\"):\n",
        "        super().__init__(name=name)\n",
        "        self.n_assets = n_assets\n",
        "        self.seq_len = seq_len\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoding = get_positional_encoding(seq_len, d_model)\n",
        "\n",
        "        # input projection\n",
        "        self.input_proj = layers.Dense(d_model, name=\"input_projection\")\n",
        "\n",
        "        # encoder layers\n",
        "        self.enc_layers = [InformerEncoderLayer(d_model, num_heads, dff, dropout)\n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        # output heads\n",
        "        self.mean_head = layers.Dense(n_assets, name=\"mean_head\")\n",
        "        self.var_head = layers.Dense(n_assets, activation=\"softplus\", name=\"var_head\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.input_proj(inputs)\n",
        "        x += self.pos_encoding[:, :self.seq_len, :]  # add positional encoding\n",
        "\n",
        "        for layer in self.enc_layers:\n",
        "            x = layer(x, training=training)\n",
        "\n",
        "        last = x[:, -1, :]\n",
        "        mean_out = self.mean_head(last)\n",
        "        var_out = self.var_head(last)\n",
        "        return {\"mean_head\": mean_out, \"var_head\": var_out}\n",
        "\n",
        "    def compile(self, optimizer=\"adam\", loss=None, metrics=None, **kwargs):\n",
        "        if loss is None:\n",
        "            loss = {\"mean_head\": \"mse\", \"var_head\": \"mse\"}\n",
        "        super().compile(optimizer=optimizer, loss=loss, metrics=metrics, **kwargs)\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper\n",
        "# -------------------------\n",
        "def build_transformer_for_env(env, n_assets=18, seq_len=60,\n",
        "                                    d_model=64, num_heads=4, dff=128,\n",
        "                                    num_layers=2, dropout=0.1, model_path=None):\n",
        "    model = TransformerModel(n_assets=n_assets, seq_len=seq_len,\n",
        "                            d_model=d_model, num_heads=num_heads,\n",
        "                            dff=dff, num_layers=num_layers, dropout=dropout)\n",
        "    # build with dummy input\n",
        "    dummy = tf.zeros((1, seq_len, n_assets))\n",
        "    _ = model(dummy)\n",
        "\n",
        "    # compile\n",
        "    model.compile(optimizer=\"adam\",\n",
        "                  loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "                  metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"})\n",
        "    env.transformer_model = model\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded short-sequence Informer+PE weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load weights from {model_path}: {e}\\n→ Starting fresh.\")\"\"\"\n",
        "\n",
        "\n",
        "class CryptoEnvironment:\n",
        "    def __init__(self, prices='/content/drive/MyDrive/crypto_portfolionew.csv', transformer=None, seq_len=60, n_assets=18, online_train=True, patience=5, batch_size=16, model_path='best_transformer_online.weights.h5', asset_cols=None):\n",
        "        self.prices = prices\n",
        "        self.capital = 1000.0\n",
        "        self.data = self.load_data()  # implement your load_data method earlier\n",
        "        self.seq_len = seq_len\n",
        "        self.n_assets = n_assets\n",
        "        self.online_train = online_train\n",
        "        self.patience = patience\n",
        "        self.batch_size = batch_size\n",
        "        self.model_path = model_path\n",
        "        self.asset_cols = asset_cols or [\n",
        "            '<CLOSE>_BANK','<CLOSE>_BMLT','<CLOSE>_DTIP','<CLOSE>_FOLD','<CLOSE>_GDIR',\n",
        "            '<CLOSE>_MADN','<CLOSE>_MAPN','<CLOSE>_MSMI','<CLOSE>_NORI','<CLOSE>_OIMC',\n",
        "            '<CLOSE>_PASN','<CLOSE>_PKLJ','<CLOSE>_PNES','<CLOSE>_PTAP','<CLOSE>_SFKZ',\n",
        "            '<CLOSE>_SSAP','<CLOSE>_TAMN','<CLOSE>_TSBE'\n",
        "        ]\n",
        "\n",
        "        # attach or build transformer (single source of truth)\n",
        "        if transformer is None:\n",
        "            build_transformer_for_env(self, n_assets=self.n_assets, seq_len=self.seq_len, model_path=self.model_path)\n",
        "        else:\n",
        "            self.transformer_model = transformer\n",
        "            # ensure built & compiled\n",
        "            _ = self.transformer_model(tf.zeros((1, self.seq_len, self.n_assets)))\n",
        "            self.transformer_model.compile(optimizer='adam', loss={'mean_head':'mse','var_head':'mse'})\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"if transformer_model is not None and os.path.exists(self.model_path):\n",
        "           try:\n",
        "              self.transformer_model.load_weights(self.model_path)\n",
        "           except Exception:\n",
        "                pass\"\"\"\n",
        "\n",
        "    def load_data(self):\n",
        "        data =  pd.read_csv(self.prices)\n",
        "        try:\n",
        "            data.index = data['Date']\n",
        "            data = data.drop(columns = ['Date'])\n",
        "        except:\n",
        "            multi_cols = pd.MultiIndex.from_arrays([\n",
        "                     data.columns,\n",
        "                     data.iloc[0],\n",
        "                     data.iloc[1]])\n",
        "            data.columns = multi_cols\n",
        "\n",
        "            # Remove the first two rows since they became the new header levels\n",
        "            data=data.iloc[2:].reset_index(drop=True)\n",
        "            data.columns = ['_'.join(map(str, col)).strip() for col in data.columns]\n",
        "            data=data.set_index('Unnamed: 0_<COL11>_Date')\n",
        "            data.columns = (data.columns.str.replace(r'[\\d.]+', '', regex=True).str.replace(r'nan', '', case=False, regex=True).str.replace(r'_+', '_', regex=True).str.strip('_')\n",
        ")\n",
        "        return data\n",
        "\n",
        "    def preprocess_state(self, state):\n",
        "\n",
        "        state = state.replace([np.inf, -np.inf], np.nan)\n",
        "        state = state.fillna(0)\n",
        "        scaler = MinMaxScaler(feature_range=(0,1))\n",
        "         # scaling dataset\n",
        "        state = pd.DataFrame(scaler.fit_transform(state),index=state.index, columns=state.columns)\n",
        "        return state\n",
        "\n",
        "    def get_state(self, t, lookback, is_cov_matrix = False, is_raw_time_series = True):\n",
        "\n",
        "        assert lookback <= t\n",
        "\n",
        "        decision_making_state = self.data.iloc[t-lookback:t]\n",
        "        decision_making_state = decision_making_state\n",
        "        #set_trace()\n",
        "        if is_cov_matrix:\n",
        "            x = decision_making_state.cov()\n",
        "            scaler =MinMaxScaler(feature_range=(0,1))\n",
        "            # scaling dataset\n",
        "            x =  pd.DataFrame(scaler.fit_transform(x),index=x.index, columns=x.columns)\n",
        "            '''retsmean=decision_making_state.mean()\n",
        "            rets12=decision_making_state-retsmean\n",
        "            lenx=rets12.shape[0]\n",
        "            leny=rets12.shape[1]\n",
        "            for i in range(lenx):\n",
        "                for j in range(leny):\n",
        "                    if (rets12.iloc[i,j]>0):\n",
        "                        rets12.iloc[i,j]=0\n",
        "            semicov=((rets12.T.dot(rets12))/(lenx-1))\n",
        "            x=semicov'''\n",
        "            return x\n",
        "        elif not is_cov_matrix and not is_raw_time_series:\n",
        "            x = decision_making_state.cov()\n",
        "            scaler =MinMaxScaler(feature_range=(0,1))\n",
        "            # scaling dataset\n",
        "            #x =  scaler.fit_transform(x)\n",
        "            decision_making_state = self.data.iloc[t-lookback:t]\n",
        "            xx = decision_making_state.pct_change().dropna()\n",
        "            #xx =  scaler.fit_transform(xx)\n",
        "            xxx =  scaler.fit_transform(np.dot(xx,x))\n",
        "            xxx= pd.DataFrame(xxx,index=xx.index, columns=xx.columns)\n",
        "\n",
        "            return xxx\n",
        "        else:\n",
        "            if is_raw_time_series:\n",
        "\n",
        "                decision_making_state = self.data.iloc[t-lookback:t]\n",
        "                decision_making_state= decision_making_state.apply(pd.to_numeric, errors='coerce').drop(columns=['Unnamed: 0'], errors='ignore').dropna()\n",
        "                returnsren =decision_making_state.iloc[:, :95].pct_change()\n",
        "                cols_to_replace = decision_making_state.columns[:95]\n",
        "                decision_making_state[cols_to_replace] = returnsren[cols_to_replace].values\n",
        "                col_to_move = \"<CLOSE>_IDXS\"\n",
        "                close_cols = [col for col in decision_making_state.columns if col.startswith(\"<CLOSE>\") and col != col_to_move]\n",
        "                if close_cols:\n",
        "                         last_close_index = decision_making_state.columns.get_loc(close_cols[-1])\n",
        "                else:\n",
        "                         last_close_index = 0\n",
        "\n",
        "                cols = list(decision_making_state.columns)\n",
        "                cols.remove(col_to_move)\n",
        "                cols.insert(last_close_index + 1, col_to_move)\n",
        "                decision_making_state = decision_making_state[cols]\n",
        "\n",
        "            return self.preprocess_state(decision_making_state.dropna())\n",
        "\n",
        "\n",
        "    def compute_BL_anchor(self, t, lookback, tau, delta):\n",
        "        \"\"\"\n",
        "        Compute a Black-Litterman anchor weights vector using a rolling window ending at index t.\n",
        "        Returns a long-only weights vector that sums to 1.\n",
        "        \"\"\"\n",
        "        # ensure enough history\n",
        "        start_idx = max(0, t - lookback)\n",
        "        window = self.data.iloc[start_idx:t][['<CLOSE>_BANK','<CLOSE>_BMLT','<CLOSE>_DTIP',\n",
        "                                                    '<CLOSE>_FOLD','<CLOSE>_GDIR','<CLOSE>_MADN',\n",
        "                                                    '<CLOSE>_MAPN','<CLOSE>_MSMI','<CLOSE>_NORI',\n",
        "                                                    '<CLOSE>_OIMC','<CLOSE>_PASN','<CLOSE>_PKLJ',\n",
        "                                                    '<CLOSE>_PNES','<CLOSE>_PTAP','<CLOSE>_SFKZ',\n",
        "                                                    '<CLOSE>_SSAP','<CLOSE>_TAMN','<CLOSE>_TSBE']].astype(float).pct_change().dropna()\n",
        "\n",
        "\n",
        "        \"\"\"if window.shape[0] < 5:\n",
        "            # fallback to equal weights if not enough data\n",
        "            n = self.data.shape[1]\n",
        "            return np.ones(n) / n\"\"\"\n",
        "\n",
        "        # robust covariance (Ledoit-Wolf shrinkage)\n",
        "        try:\n",
        "            lw = LedoitWolf().fit(window.values)\n",
        "            Sigma = lw.covariance_\n",
        "        except Exception:\n",
        "            Sigma = window.cov().values\n",
        "\n",
        "        n = Sigma.shape[0]\n",
        "\n",
        "        # Market implied returns (simple reverse optimization using equal market weights)\n",
        "\n",
        "        index_returns = self.data['<CLOSE>_IDXS'].astype(float).pct_change().dropna().values\n",
        "        # align sizes\n",
        "        min_len = min(len(window.values), len(index_returns))\n",
        "        X = window.values[-min_len:]\n",
        "        y = index_returns[-min_len:]\n",
        "\n",
        "        # --- Step B: constrained regression (cvxpy) ---\n",
        "        w = cp.Variable(n)\n",
        "        objective = cp.Minimize(0.5*cp.sum_squares(X @ w - y))\n",
        "        constraints = [cp.sum(w) == 1, w >= 0]\n",
        "        prob = cp.Problem(objective, constraints)\n",
        "        prob.solve()\n",
        "\n",
        "        \"\"\"if w.value is None:\n",
        "            # fallback: equal weights\n",
        "            market_weights = np.zeros(n) / n\n",
        "        else:\"\"\"\n",
        "        market_weights = np.array(w.value).flatten()\n",
        "\n",
        "        #market_weights = np.ones(n) / n\n",
        "        pi = delta * Sigma.dot(market_weights)   # implied equilibrium returns\n",
        "\n",
        "        n_assets = 18  # 18 assets\n",
        "        seq_len=60\n",
        "        # Transformer prediction\n",
        "        if self.transformer_model is not None and window.shape[0] >= self.seq_len:\n",
        "            x_input = tf.convert_to_tensor(window.values[-self.seq_len:][np.newaxis, ...], dtype=tf.float32)\n",
        "            #model = self.transformer_model(n_assets=n_assets, seq_len=seq_len)\n",
        "            #outputs = model(x_input, training=False)\n",
        "            outputs = self.transformer_model(x_input, training=False)\n",
        "            mu_pred = outputs[\"mean_head\"]\n",
        "            var_pred = outputs[\"var_head\"]\n",
        "\n",
        "            Q = mu_pred.numpy().flatten()\n",
        "            view_var = var_pred.numpy().flatten()\n",
        "        \"\"\"else:\n",
        "            Q = window.mean().values\n",
        "            view_var = window.var().values\"\"\"\n",
        "\n",
        "        # P = identity (one view per asset)\n",
        "        P = np.eye(n)\n",
        "\n",
        "        # Build Omega (diagonal) from view variances (scaled)\n",
        "        # Avoid zeros: use variance of returns * small factor\n",
        "        #view_var = window.var().values\n",
        "        # scale factor to ensure Omega not too large/small\n",
        "        omega_scale = 0.1\n",
        "        Omega = np.diag(np.maximum(view_var * omega_scale, 1e-8))\n",
        "\n",
        "        # Black-Litterman posterior expected returns\n",
        "        tauSigma = tau * Sigma\n",
        "        # compute inverse parts safely with pseudo-inverse fallback\n",
        "        try:\n",
        "            inv_tauSigma = np.linalg.inv(tauSigma)\n",
        "        except np.linalg.LinAlgError:\n",
        "            inv_tauSigma = np.linalg.pinv(tauSigma)\n",
        "        try:\n",
        "            Omega_inv = np.linalg.inv(Omega)\n",
        "        except np.linalg.LinAlgError:\n",
        "            Omega_inv = np.linalg.pinv(Omega)\n",
        "\n",
        "        A = inv_tauSigma + P.T @ Omega_inv @ P\n",
        "        try:\n",
        "            A_inv = np.linalg.inv(A)\n",
        "        except np.linalg.LinAlgError:\n",
        "            A_inv = np.linalg.pinv(A)\n",
        "\n",
        "        right = inv_tauSigma @ pi + P.T @ Omega_inv @ Q\n",
        "        mu_bl = A_inv @ right\n",
        "\n",
        "        # Convert posterior returns to mean-variance weights: w = inv(delta*Sigma) * mu\n",
        "        try:\n",
        "            inv_deltaSigma = np.linalg.inv(delta * Sigma)\n",
        "        except np.linalg.LinAlgError:\n",
        "            inv_deltaSigma = np.linalg.pinv(delta * Sigma)\n",
        "        raw_w = inv_deltaSigma @ mu_bl\n",
        "\n",
        "        # Project to long-only simplex (non-negative, sum to 1)\n",
        "        raw_w = np.maximum(raw_w, 0.0)\n",
        "        s = raw_w.sum()\n",
        "        \"\"\"if s <= 0:\n",
        "            # fallback equal weights\n",
        "            w_bl = np.ones(n) / n\n",
        "        else:\"\"\"\n",
        "        w_bl = raw_w / s\n",
        "\n",
        "        return w_bl\n",
        "\n",
        "\n",
        "        # Online training step for Transformer\n",
        "        # -------------------\n",
        "    def online_train_transformer(env, t, lookback=250, epochs=3):\n",
        "        # this function expects env.data to be a pandas DataFrame with same asset columns as used for training\n",
        "        if env.transformer_model is None or t < lookback + env.seq_len:\n",
        "            return\n",
        "\n",
        "        # build window and sequences (same logic as make_transformer_data)\n",
        "        start_idx = max(0, t - lookback - env.seq_len)\n",
        "        end_idx = t\n",
        "        asset_cols = env.asset_cols if hasattr(env, \"asset_cols\") else None\n",
        "        if asset_cols is None:\n",
        "            raise ValueError(\"env.asset_cols must be set for online_train_transformer.\")\n",
        "\n",
        "        window = env.data.iloc[start_idx:end_idx][asset_cols].astype(float).pct_change().dropna()\n",
        "        if window.shape[0] < env.seq_len + 1:\n",
        "            return\n",
        "\n",
        "        X_seq, Y_mean, Y_var = [], [], []\n",
        "        for i in range(len(window) - env.seq_len):\n",
        "            x_i = window.values[i:i+env.seq_len]\n",
        "            y_i = window.values[i+env.seq_len]\n",
        "            X_seq.append(x_i)\n",
        "            Y_mean.append(y_i)\n",
        "            Y_var.append(np.var(window.values[i:i+env.seq_len], axis=0))\n",
        "\n",
        "        X_seq = np.array(X_seq, dtype=np.float32)\n",
        "        Y_mean = np.array(Y_mean, dtype=np.float32)\n",
        "        Y_var = np.array(Y_var, dtype=np.float32)\n",
        "        if len(X_seq) < 2:\n",
        "            return\n",
        "\n",
        "        ds = tf.data.Dataset.from_tensor_slices((X_seq, {'mean_head': Y_mean, 'var_head': Y_var})).batch(env.batch_size)\n",
        "\n",
        "        early_stop = EarlyStopping(monitor='loss', patience=env.patience, restore_best_weights=True)\n",
        "        checkpoint = ModelCheckpoint(env.model_path, monitor='loss', mode='min', save_best_only=True, save_weights_only=True, verbose=1)\n",
        "\n",
        "        # compile & train\n",
        "        env.transformer_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss={'mean_head':'mse','var_head':'mse'})\n",
        "        env.transformer_model.fit(ds, epochs=epochs, callbacks=[early_stop, checkpoint], verbose=0)\n",
        "        print(\"✅ Online transformer checkpoint saved?:\", os.path.exists(env.model_path))\n",
        "\n",
        "\n",
        "    def get_reward(self, action, action_t, reward_t,W,capital,name, alpha = 0.50,\n",
        "                   bl_lookback=1000, tau=0.05, delta=2.5):\n",
        "        def local_portfolio(returns, weights):\n",
        "              weights = np.array(weights)\n",
        "              rets = returns.mean()\n",
        "              retsmean = returns.mean()\n",
        "              rets12 = returns - retsmean\n",
        "              lenx = rets12.shape[0]\n",
        "              rf = 0.21 / 252\n",
        "\n",
        "              rets12[rets12 > 0] = 0\n",
        "              semicov = ((rets12.T.dot(rets12)) / (lenx - 1))\n",
        "\n",
        "              # Portfolio metrics\n",
        "              P_ret = rets\n",
        "              P_vol = returns.std()\n",
        "              P_semivol=semicov\n",
        "              P_sharpe = (P_ret - rf) / P_vol\n",
        "              P_sortino = (P_ret - rf) / P_semivol\n",
        "              # === New Metrics ===\n",
        "              ## 1. Calmar Ratio\n",
        "              cumulative_returns = (returns + 1).cumprod()\n",
        "              running_max = cumulative_returns.cummax()\n",
        "              drawdown = (cumulative_returns - running_max) / running_max\n",
        "              max_drawdown = drawdown.min()\n",
        "\n",
        "              # daily → annualized compound return\n",
        "              annualized_return = (cumulative_returns.iloc[-1]) ** (1 / len(cumulative_returns)) - 1\n",
        "              calmar_ratio = (annualized_return-rf) / abs(max_drawdown) if max_drawdown != 0 else np.nan\n",
        "\n",
        "              ## 2. Excess Return to Value-at-Risk (VaR)\n",
        "              excess_return = returns.mean() - rf\n",
        "              var_95 = np.percentile(returns, 5)\n",
        "              excess_return_to_var = excess_return / abs(var_95) if var_95 != 0 else np.nan\n",
        "\n",
        "              # 3. Excess Return to Conditional Value-at-Risk (CVaR)\n",
        "              excess_return = returns.mean() - rf\n",
        "              var_95 = np.percentile(returns, 5)\n",
        "              cvar_95 = returns[returns <= var_95].mean()\n",
        "              excess_return_to_cvar = excess_return / abs(cvar_95) if cvar_95 != 0 else np.nan\n",
        "\n",
        "              return np.array([P_ret, P_vol,P_semivol, P_sharpe,P_sortino, calmar_ratio, excess_return_to_var,excess_return_to_cvar])\n",
        "\n",
        "        \"\"\"# Online training\n",
        "        if self.online_train:\n",
        "            self.online_train_transformer(action_t, lookback=bl_lookback)\n",
        "         # Normalize DQN action\"\"\"\n",
        "        w_action = np.array(action, dtype=float)\n",
        "        w_action = np.maximum(w_action, 0.0)\n",
        "        w_action = w_action / (w_action.sum() if w_action.sum() > 0 else len(w_action))\n",
        "        \"\"\"if w_action.sum() <= 0:\n",
        "            # fallback to equal\n",
        "            w_action = np.ones_like(w_action) / len(w_action)\n",
        "        else:\n",
        "            w_action = np.maximum(w_action, 0.0)\n",
        "            w_action = w_action / w_action.sum()\"\"\"\n",
        "\n",
        "        # Compute BL anchor at time action_t (using data ending at action_t)\n",
        "        #try:\n",
        "\n",
        "        w_bl = self.compute_BL_anchor(action_t, lookback=bl_lookback, tau=tau, delta=delta)\n",
        "        #except Exception:\n",
        "            # fallback equal weights\n",
        "            #w_bl = np.ones_like(w_action) / len(w_action)\n",
        "\n",
        "        # Blend anchor and action\n",
        "\n",
        "\n",
        "        if name=='eq':\n",
        "              w_final = w_action\n",
        "        else:\n",
        "              w_final = (1.0 - alpha) * w_bl + alpha * w_action\n",
        "        w_final = np.maximum(w_final, 0.0)\n",
        "        \"\"\"if w_final.sum() <= 0:\n",
        "            w_final = np.ones_like(w_final) / len(w_final)\n",
        "        else:\"\"\"\n",
        "        w_final = w_final / w_final.sum()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Transaction costs (approx) using your existing logic\n",
        "        cost_changes = list(np.array(w_final) - np.array(W))\n",
        "        cocd = [element * 0.00371 if element >= 0 else abs(element) * 0.0088 for element in cost_changes]\n",
        "        cocod = sum(cocd)\n",
        "\n",
        "        # Select the same price columns you used originally (safe select)\n",
        "        cols = ['<CLOSE>_BANK','<CLOSE>_BMLT','<CLOSE>_DTIP',\n",
        "                '<CLOSE>_FOLD','<CLOSE>_GDIR','<CLOSE>_MADN',\n",
        "                '<CLOSE>_MAPN','<CLOSE>_MSMI','<CLOSE>_NORI',\n",
        "                '<CLOSE>_OIMC','<CLOSE>_PASN','<CLOSE>_PKLJ',\n",
        "                '<CLOSE>_PNES','<CLOSE>_PTAP','<CLOSE>_SFKZ',\n",
        "                '<CLOSE>_SSAP','<CLOSE>_TAMN','<CLOSE>_TSBE']\n",
        "        # clamp columns to those present\n",
        "        cols_present = [c for c in cols if c in self.data.columns]\n",
        "        if len(cols_present) == 0:\n",
        "            raise ValueError(\"None of the expected price columns are in self.data\")\n",
        "\n",
        "        data_period = self.data.iloc[action_t:reward_t][cols_present].apply(pd.to_numeric, errors='coerce').dropna(how='all')\n",
        "        if data_period.shape[0] < 2:\n",
        "            # Not enough price rows, return zero reward fallback\n",
        "            returns_series = pd.Series(dtype=float)\n",
        "            sharpe_arr = np.array([0.0] * len(w_final))\n",
        "            returns=returns_series\n",
        "            sharpe=sharpe_arr\n",
        "            W=list(w_final)\n",
        "            return returns, sharpe, W, capital\n",
        "\n",
        "        # Build portfolio value series via shares method (avoids cumulative product oddities)\n",
        "        first_prices = data_period.iloc[0].values.astype(float)\n",
        "        # avoid zero division\n",
        "        first_prices[first_prices == 0] = 1e-8\n",
        "        shares = (capital * w_final) / first_prices  # number of shares or units per asset\n",
        "\n",
        "        # compute portfolio value across the window\n",
        "        prices_matrix = data_period.values.astype(float)\n",
        "        pos_values = prices_matrix * shares.reshape(1, -1)  # each row is date\n",
        "        portfolio_values = pos_values.sum(axis=1)\n",
        "\n",
        "        # cash leftover invested at rf\n",
        "        rf = 0.21 / 252.0\n",
        "        invested = (shares * first_prices).sum()\n",
        "        cash0 = capital - invested\n",
        "        # cash growth series\n",
        "        cash_series = cash0 * ((1.0 + rf) ** np.arange(len(data_period)))\n",
        "        total_series = portfolio_values + cash_series\n",
        "\n",
        "        # apply transaction cost once at the start\n",
        "        total_series[0] = total_series[0] - cocod * total_series[0]\n",
        "\n",
        "        # compute returns series (daily)\n",
        "        total_series = pd.Series(total_series, index=data_period.index)\n",
        "        returns_series = total_series.pct_change().dropna()\n",
        "        if returns_series.empty:\n",
        "            sharpe = 0.0\n",
        "        else:\n",
        "            # excess return per period: subtract rf per period\n",
        "            excess = returns_series - rf\n",
        "            # annualize-like sharpe using daily stats\n",
        "            try:\n",
        "                sharpe = (excess.mean() / (returns_series.std() + 1e-12)) * np.sqrt(252)\n",
        "            except Exception:\n",
        "                sharpe = 0.0\n",
        "\n",
        "        # match original shape behavior: return sharpe as repeated array if needed\n",
        "        weights = w_final\n",
        "        sharpe=local_portfolio(returns_series, weights)[3]\n",
        "        sharpe_arr = np.array([sharpe] * len(self.data.columns))\n",
        "\n",
        "        final_capital = float(total_series.iloc[-1])\n",
        "        returns=returns_series\n",
        "        sharpe=sharpe_arr\n",
        "        W=list(w_final)\n",
        "        capital=final_capital\n",
        "        # return consistent signature with your code\n",
        "\n",
        "\n",
        "        #rew = (data_period.values[-1] - data_period.values[0]) / data_period.values[0]\n",
        "\n",
        "        # === New Metrics ===\n",
        "        ## 1. Calmar Ratio\n",
        "        \"\"\"cumulative_returns = kll['port'] / kll['port'].iloc[0]\n",
        "        running_max = cumulative_returns.cummax()\n",
        "        drawdown = (cumulative_returns - running_max) / running_max\n",
        "        max_drawdown = drawdown.min()\n",
        "        annualized_return = (cumulative_returns.iloc[-1]) ** (1 / len(cumulative_returns)) - 1\n",
        "        sharpe = (annualized_return - rf)/ abs(max_drawdown) if max_drawdown != 0 else np.nan\n",
        "        sharpe = np.array([sharpe] * (len(self.data.columns)-191))\n",
        "\n",
        "        ## 2. Excess Return to Value-at-Risk (VaR)\n",
        "        excess_return = returns.mean() - rf\n",
        "        var_95 = np.percentile(returns, 5)\n",
        "        sharpe = excess_return / abs(var_95) if var_95 != 0 else np.nan\n",
        "        sharpe = np.array([sharpe] * (len(self.data.columns)-191))\"\"\"\n",
        "        return returns, sharpe, W, capital,\n",
        "\n"
      ],
      "metadata": {
        "id": "Edc8zNnNY8DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D,GlobalMaxPooling1D, Dense, Dropout, Flatten, LSTM, LayerNormalization, MultiHeadAttention, Add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Permute, Softmax, Multiply\n",
        "from tensorflow.keras.layers import Dense, Multiply, Lambda,Embedding\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten,\n",
        "    LayerNormalization, MultiHeadAttention, Add\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Layer\n",
        "import random\n",
        "from collections import deque\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"class Attention(tf.keras.layers.Layer):\n",
        "          def __init__(self, units):\n",
        "              super(Attention, self).__init__()\n",
        "              self.W = tf.keras.layers.Dense(units)\n",
        "              self.V = tf.keras.layers.Dense(1)\n",
        "          def call(self, inputs):\n",
        "              # Compute attention scores\n",
        "              score = tf.nn.tanh(self.W(inputs))\n",
        "              attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "              # Apply attention weights to input\n",
        "              context_vector = attention_weights * inputs\n",
        "              context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "              return context_vector\"\"\"\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, portfolio_size, is_eval=False, allow_short=False, buffer_size=50000):\n",
        "        self.portfolio_size = portfolio_size\n",
        "        self.allow_short = allow_short\n",
        "        self.input_shape = (199, 209)\n",
        "        self.action_size = 3  # sit, buy, sell\n",
        "\n",
        "        # bounded replay buffer (prevents Colab hanging)\n",
        "        self.memory4replay = deque(maxlen=buffer_size)\n",
        "\n",
        "        self.is_eval = is_eval\n",
        "\n",
        "        self.alpha = 0.85\n",
        "        self.gamma = 0.50\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "\n",
        "        self.model = self._model()\n",
        "\n",
        "    \"\"\"def _model(self):\n",
        "        inputs = Input(shape=self.input_shape)\n",
        "\n",
        "        x = Conv1D(32, kernel_size=2, activation=\"relu\", padding=\"same\")(inputs)\n",
        "        x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "        x = Conv1D(32, kernel_size=2, activation=\"relu\", padding=\"same\")(x)\n",
        "        x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(32, activation='elu')(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "\n",
        "        predictions = [Dense(self.action_size, activation='linear')(x) for _ in range(self.portfolio_size - 191)]\n",
        "        model = Model(inputs=inputs, outputs=predictions)\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        return model\"\"\"\n",
        "\n",
        "    \"\"\"def _model(self):\n",
        "            inputs = Input(shape=self.input_shape)\n",
        "\n",
        "            x = Conv1D(64, kernel_size=2, activation=\"relu\", padding=\"same\")(inputs)\n",
        "            x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "            x = LSTM(64, return_sequences=False)(x)\n",
        "            x = Dense(64, activation='elu')(x)\n",
        "            x = Dropout(0.4)(x)\n",
        "\n",
        "            predictions = [Dense(self.action_size, activation='linear')(x) for _ in range(self.portfolio_size-191)]\n",
        "            model = Model(inputs=inputs, outputs=predictions)\n",
        "            model.compile(optimizer='adam', loss='mse')\n",
        "            return model\"\"\"\n",
        "\n",
        "    def _transformer_encoder(self, x, head_size=4, num_heads=2, ff_dim=64, dropout=0.4):\n",
        "            attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(x, x)\n",
        "            attn_output = Dropout(dropout)(attn_output)\n",
        "            x = Add()([x, attn_output])\n",
        "            x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "            ffn = Dense(ff_dim, activation=\"relu\")(x)\n",
        "            ffn = Dense(x.shape[-1])(ffn)\n",
        "            x = Add()([x, ffn])\n",
        "            x = LayerNormalization(epsilon=1e-6)(x)\n",
        "            return x\n",
        "\n",
        "    class SinusoidalPositionalEncoding(tf.keras.layers.Layer):\n",
        "            def __init__(self, max_len, d_model):\n",
        "                super().__init__()\n",
        "                pos = np.arange(max_len)[:, np.newaxis]\n",
        "                i = np.arange(d_model)[np.newaxis, :]\n",
        "                angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "                angle_rads = pos * angle_rates\n",
        "                angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "                angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "                self.pos_encoding = tf.constant(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
        "\n",
        "            def call(self, x):\n",
        "                seq_len = tf.shape(x)[1]\n",
        "                return x + self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    def _model(self):\n",
        "            inputs = Input(shape=self.input_shape)\n",
        "\n",
        "            x = Dense(64)(inputs)  # Project features to d_model dimension\n",
        "            x = self.SinusoidalPositionalEncoding(199, 64)(x)\n",
        "            x = self._transformer_encoder(x)\n",
        "\n",
        "            x = GlobalMaxPooling1D()(x)\n",
        "            x = Dense(64, activation='elu')(x)\n",
        "            x = Dropout(0.4)(x)\n",
        "\n",
        "            predictions = [Dense(self.action_size, activation='linear')(x) for _ in range(self.portfolio_size-191)]\n",
        "            model = Model(inputs=inputs, outputs=predictions)\n",
        "            model.compile(optimizer='adam', loss='mse')\n",
        "            return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"def attention_block(self,x):\n",
        "       # x: (batch, timesteps, features)\n",
        "       attention = Dense(1, activation='tanh')(x)  # (batch, timesteps, 1)\n",
        "       attention = Lambda(lambda a: tf.nn.softmax(a, axis=1))(attention)  # softmax over time\n",
        "       x = Multiply()([x, attention])  # (batch, timesteps, features)\n",
        "       return x\n",
        "\n",
        "    def _model(self):\n",
        "        inputs = Input(shape=self.input_shape)\n",
        "\n",
        "        x = Conv1D(filters=128, kernel_size=2, activation='relu', padding='valid')(inputs)\n",
        "        x = MaxPooling1D(pool_size=2, strides=1)(x)\n",
        "        x = self.attention_block(x)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(128, activation='elu')(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(64, activation='elu')(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "        predictions = [Dense(self.action_size, activation='linear')(x) for _ in range(self.portfolio_size-191)]\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=predictions)\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        return model\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"def _model(self):\n",
        "\n",
        "        inputs = Input(shape=self.input_shape)\n",
        "        #Flatten()\n",
        "        #x = Flatten()(inputs)\n",
        "        x=keras.layers.Conv1D(filters=252, kernel_size=2,\n",
        "                      strides=1, padding=\"valid\",\n",
        "                      activation=\"relu\")(inputs)\n",
        "        #x =Flatten()(x)\n",
        "\n",
        "        x=keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=\"valid\")(x)\n",
        "        x=keras.layers.Conv1D(filters=252, kernel_size=2,\n",
        "                      strides=1, padding=\"valid\",\n",
        "                      activation=\"relu\")( inputs)\n",
        "        x=keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=\"valid\")(x)\n",
        "        x=keras.layers.Conv1D(filters=252, kernel_size=2,\n",
        "                      strides=1, padding=\"valid\",\n",
        "                      activation=\"relu\")( inputs)\n",
        "        x=keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=\"valid\")(x)\n",
        "\n",
        "\n",
        "        #x=keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "                              # backward_layer=tf.keras.layers.LSTM(64, activation='relu', return_sequences=True, go_backwards=True),\n",
        "                              #merge_mode=\"concat\")(x)\n",
        "        #x=keras.layers.Conv1D(filters=32, kernel_size=2,\n",
        "                      #strides=1, padding=\"valid\",\n",
        "                      #activation=\"relu\")(x)\n",
        "       # x=keras.layers.MaxPooling1D(pool_size=2,strides=2, padding=\"valid\")(x)\n",
        "        #Attention(64),\n",
        "        #tf.keras.layers.Reshape((128,1)),\n",
        "        #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(126, return_sequences=True),\n",
        "                                 #backward_layer=tf.keras.layers.LSTM(126, activation='relu', return_sequences=True, go_backwards=True),\n",
        "                                 #merge_mode=\"concat\"),\n",
        "        x = Dense(256, activation='elu')(x)\n",
        "\n",
        "        #\n",
        "        #x=keras.layers.LSTM(256,activation='relu',return_sequences=False)(x)\n",
        "        #x=keras.layers.Reshape((256,1))(x)\n",
        "        #x=keras.layers.LSTM(30,activation='relu',return_sequences=False)(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(256, activation='elu')(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        predictions = []\n",
        "        for i in range(self.portfolio_size-191):\n",
        "            asset_dense = Dense(self.action_size, activation='linear')(x)\n",
        "            predictions.append(asset_dense)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=predictions)\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        return model\"\"\"\n",
        "\n",
        "    \"\"\"def _model(self):\n",
        "           inputs = Input(shape=self.input_shape)\n",
        "\n",
        "           x = Conv1D(filters=128, kernel_size=2, activation='relu', padding='valid')(inputs)\n",
        "           x = MaxPooling1D(pool_size=2, strides=1)(x)\n",
        "           x = LSTM(128, return_sequences=False)(x)\n",
        "\n",
        "           x = Dense(128, activation='elu')(x)\n",
        "           x = Dropout(0.5)(x)\n",
        "           x = Dense(64, activation='elu')(x)\n",
        "           x = Dropout(0.5)(x)\n",
        "\n",
        "           x = Flatten()(x)\n",
        "           predictions = [Dense(self.action_size, activation='linear')(x) for _ in range(self.portfolio_size-191)]\n",
        "\n",
        "           model = Model(inputs=inputs, outputs=predictions)\n",
        "           model.compile(optimizer='adam', loss='mse')\n",
        "           return model\"\"\"\n",
        "\n",
        "\n",
        "    \"\"\"def _transformer_encoder(self, x, head_size=4, num_heads=4, ff_dim=8, dropout=0.5):\n",
        "        # Multi-head attention block\n",
        "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(x, x)\n",
        "        attn_output = Dropout(dropout)(attn_output)\n",
        "        x = Add()([x, attn_output])\n",
        "        x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Feed-forward block\n",
        "        ffn = Dense(ff_dim, activation=\"relu\")(x)\n",
        "        ffn = Dense(x.shape[-1])(ffn)\n",
        "        x = Add()([x, ffn])\n",
        "        x = LayerNormalization(epsilon=1e-6)(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def _model(self):\n",
        "        inputs = Input(shape=self.input_shape)\n",
        "        x = Conv1D(32, 3, activation=\"relu\", padding=\"same\")(inputs)\n",
        "        x = Conv1D(16, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "\n",
        "        x = self._transformer_encoder(x)  # encoder with default config\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(128, activation='elu')(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "        x = Dense(64, activation='elu')(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "\n",
        "        predictions = [Dense(self.action_size, activation='linear')(x) for _ in range(self.portfolio_size - 191)]\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=predictions)\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        return model\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"def attention_block(self,x):\n",
        "       # x: (batch, timesteps, features)\n",
        "       attention = Dense(1, activation='tanh')(x)  # (batch, timesteps, 1)\n",
        "       attention = Lambda(lambda a: tf.nn.softmax(a, axis=1))(attention)  # softmax over time\n",
        "       x = Multiply()([x, attention])  # (batch, timesteps, features)\n",
        "       return x\n",
        "\n",
        "    def _model(self):\n",
        "        inputs = Input(shape=self.input_shape)\n",
        "\n",
        "        x = Conv1D(filters=128, kernel_size=2, activation='relu', padding='valid')(inputs)\n",
        "        x = MaxPooling1D(pool_size=2, strides=1)(x)\n",
        "        x = self.attention_block(x)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(128, activation='elu')(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(64, activation='elu')(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "        predictions = [Dense(self.action_size, activation='linear')(x) for _ in range(self.portfolio_size-191)]\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=predictions)\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        return model\"\"\"\n",
        "\n",
        "\n",
        "    \"\"\"class SinusoidalPositionalEncoding(Layer):\n",
        "        def __init__(self, max_len, d_model):\n",
        "            super().__init__()\n",
        "            self.max_len = max_len\n",
        "            self.d_model = d_model\n",
        "\n",
        "            # Create positional encodings once at initialization\n",
        "            pos = np.arange(max_len)[:, np.newaxis]\n",
        "            i = np.arange(d_model)[np.newaxis, :]\n",
        "            angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "            angle_rads = pos * angle_rates\n",
        "\n",
        "            # Apply sin to even indices and cos to odd indices\n",
        "            angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "            angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "            # Convert to constant tensor\n",
        "            self.pos_encoding = tf.constant(angle_rads[np.newaxis, ...], dtype=tf.float32)  # shape (1, max_len, d_model)\n",
        "\n",
        "        def call(self, x):\n",
        "            seq_len = tf.shape(x)[1]\n",
        "            return x + self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    class PositionalEmbedding(Layer):\n",
        "        def __init__(self, max_len, d_model):\n",
        "            super().__init__()\n",
        "            self.pos_embedding = Embedding(input_dim=max_len, output_dim=d_model)\n",
        "\n",
        "        def call(self, x):\n",
        "            positions = tf.range(start=0, limit=tf.shape(x)[1])\n",
        "            pos_encoding = self.pos_embedding(positions)\n",
        "            return x + pos_encoding\n",
        "\n",
        "    def _transformer_encoder(self, x, head_size=16, num_heads=4, ff_dim=64, dropout=0.2):\n",
        "        # Multi-head attention block\n",
        "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(x, x)\n",
        "        attn_output = Dropout(dropout)(attn_output)\n",
        "        x = Add()([x, attn_output])\n",
        "        x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Feed-forward block\n",
        "        ffn = Dense(ff_dim, activation=\"relu\")(x)\n",
        "        ffn = Dense(x.shape[-1])(ffn)\n",
        "        x = Add()([x, ffn])\n",
        "        x = LayerNormalization(epsilon=1e-6)(x)\n",
        "        return x\n",
        "\n",
        "    def _model(self):\n",
        "        inputs = Input(shape=self.input_shape)  # (timesteps, features)\n",
        "\n",
        "        # 1. CNN\n",
        "        x = Conv1D(filters=32, kernel_size=2, activation='relu', padding='same')(inputs)\n",
        "        x = MaxPooling1D(pool_size=2, strides=1)(x)\n",
        "\n",
        "        # 2. LSTM\n",
        "        x = LSTM(16, return_sequences=True)(x)\n",
        "\n",
        "\n",
        "        sequence_length = self.input_shape[0]\n",
        "        d_model = x.shape[-1]\n",
        "        positions = tf.expand_dims(tf.range(start=0, limit=sequence_length, delta=1), 0)  # shape (1, time)\n",
        "        pos_encoding = Embedding(input_dim=1000, output_dim=d_model)(positions)           # shape (1, time, d_model)\n",
        "        x = x + pos_encoding\n",
        "\n",
        "        #x = self.PositionalEmbedding(max_len=1000, d_model=x.shape[-1])(x)\n",
        "        x =self.SinusoidalPositionalEncoding(max_len=1000, d_model=x.shape[-1])(x)\n",
        "\n",
        "\n",
        "        # 3. Transformer encoder\n",
        "        x = self._transformer_encoder(x)\n",
        "\n",
        "        # 4. Dense layers\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(32, activation='elu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Dense(16, activation='elu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "\n",
        "        # 5. Multi-head output (one head per asset)\n",
        "        outputs = [Dense(self.action_size, activation='linear')(x) for _ in range(self.portfolio_size-191)]\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        return model\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    def nn_pred_to_weights(self, pred, allow_short=False):\n",
        "        weights = np.zeros(len(pred))\n",
        "        raw_weights = np.argmax(pred, axis=-1)\n",
        "\n",
        "        saved_min = None\n",
        "        for e, r in enumerate(raw_weights):\n",
        "            if r == 0:  # sit\n",
        "                weights[e] = 0\n",
        "            elif r == 1:  # buy\n",
        "                weights[e] = np.abs(pred[e][0][r])\n",
        "            else:       # sell\n",
        "                weights[e] = -np.abs(pred[e][0][r])\n",
        "\n",
        "        if not allow_short:\n",
        "            weights += np.abs(np.min(weights))\n",
        "            saved_min = np.abs(np.min(weights))\n",
        "            saved_sum = np.sum(weights)\n",
        "        else:\n",
        "            saved_sum = np.sum(np.abs(weights))\n",
        "\n",
        "        weights /= saved_sum\n",
        "        return weights, saved_min, saved_sum\n",
        "\n",
        "    def act(self, state):\n",
        "        if not self.is_eval and random.random() <= self.epsilon:\n",
        "            w = np.random.normal(0, 1, size=(self.portfolio_size - 191,))\n",
        "            saved_min = None\n",
        "            if not self.allow_short:\n",
        "                w += np.abs(np.min(w))\n",
        "                saved_min = np.abs(np.min(w))\n",
        "            saved_sum = np.sum(w)\n",
        "            w /= saved_sum\n",
        "            return w, saved_min, saved_sum\n",
        "\n",
        "        pred = self.model.predict(np.expand_dims(state, 0), verbose=0)\n",
        "        return self.nn_pred_to_weights(pred, self.allow_short)\n",
        "\n",
        "    def remember(self, s, s_, action, reward, done):\n",
        "        self.memory4replay.append((s, s_, action, reward, done))\n",
        "\n",
        "    def expReplay(self, batch_size):\n",
        "        if len(self.memory4replay) < batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory4replay, batch_size)\n",
        "\n",
        "        def weights_to_nn_preds_with_reward(action_weights, reward,\n",
        "                                            Q_star=np.zeros((self.portfolio_size - 191, self.action_size))):\n",
        "            Q = np.zeros((self.portfolio_size - 191, self.action_size))\n",
        "            for i in range(self.portfolio_size - 191):\n",
        "                if action_weights[i] == 0:\n",
        "                    Q[i][0] = reward[i] + self.gamma * np.max(Q_star[i][0])\n",
        "                elif action_weights[i] > 0:\n",
        "                    Q[i][1] = reward[i] + self.gamma * np.max(Q_star[i][1])\n",
        "                else:\n",
        "                    Q[i][2] = reward[i] + self.gamma * np.max(Q_star[i][2])\n",
        "            return Q\n",
        "\n",
        "        def restore_Q_from_weights_and_stats(action):\n",
        "            action_weights, action_min, action_sum = action[0], action[1], action[2]\n",
        "            action_weights = action_weights * action_sum\n",
        "            if action_min is not None:\n",
        "                action_weights = action_weights - action_min\n",
        "            return action_weights\n",
        "\n",
        "        for (s, s_, action, reward, done) in minibatch:\n",
        "            action_weights = restore_Q_from_weights_and_stats(action)\n",
        "            Q_learned_value = weights_to_nn_preds_with_reward(action_weights, reward)\n",
        "            s, s_ = s.values, s_.values\n",
        "\n",
        "            if not done:\n",
        "                Q_star = self.model.predict(np.expand_dims(s_, 0), verbose=0)\n",
        "                Q_learned_value = weights_to_nn_preds_with_reward(action_weights, reward, np.squeeze(Q_star))\n",
        "\n",
        "            Q_learned_value = [xi.reshape(1, -1) for xi in Q_learned_value]\n",
        "            Q_current_value = self.model.predict(np.expand_dims(s, 0), verbose=0)\n",
        "            Q = [np.add(a * (1 - self.alpha), q * self.alpha)\n",
        "                 for a, q in zip(Q_current_value, Q_learned_value)]\n",
        "\n",
        "            self.model.fit(np.expand_dims(s, 0), Q, epochs=1, verbose=0)\n",
        "\n",
        "        # decay epsilon once per replay call\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "A76KnsLfY8F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_ASSETS = 209 #53\n",
        "agent = Agent(N_ASSETS)\n",
        "env = CryptoEnvironment()\n",
        "window_size = 200\n",
        "episode_count =1\n",
        "batch_size = 32\n",
        "rebalance_period = 5 #every 90 days weight change"
      ],
      "metadata": {
        "id": "CC_lVJpyY8Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# ✅ EarlyStopping for RL\n",
        "# -------------------------------\n",
        "class EarlyStoppingRL:\n",
        "    def __init__(self, monitor='reward', mode='max', patience=10, min_delta=1e-3, verbose=True):\n",
        "        self.monitor = monitor\n",
        "        self.mode = mode\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.best_score = -np.inf if mode == 'max' else np.inf\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.stop_training = False\n",
        "        self.best_episode = 0\n",
        "\n",
        "    def check(self, current_value, episode):\n",
        "        if self.mode == 'max':\n",
        "            improvement = current_value - self.best_score\n",
        "        else:\n",
        "            improvement = self.best_score - current_value\n",
        "\n",
        "        if improvement > self.min_delta:\n",
        "            self.best_score = current_value\n",
        "            self.best_episode = episode\n",
        "            self.wait = 0\n",
        "            if self.verbose:\n",
        "                print(f\"→ {self.monitor} improved to {current_value:.4f} at episode {episode}\")\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.verbose:\n",
        "                print(f\"No significant improvement in {self.monitor} at episode {episode} (wait={self.wait})\")\n",
        "\n",
        "            if self.wait >= self.patience:\n",
        "                self.stop_training = True\n",
        "                self.stopped_epoch = episode\n",
        "                if self.verbose:\n",
        "                    print(f\"✋ Early stopping triggered at episode {episode}. \"\n",
        "                          f\"Best {self.monitor} = {self.best_score:.4f} at episode {self.best_episode}\")\n",
        "        return self.stop_training\n",
        " # -------------------------\n",
        "# Builder wrapper (keeps same naming across train/eval)\n",
        "# -------------------------\n",
        "\"\"\"def build_transformer_for_env(env, n_assets=18, seq_len=60, d_model=64, num_heads=4, dff=128, num_layers=2, dropout=0.1, model_path=None):\n",
        "    model = TransformerModel(n_assets=n_assets, seq_len=seq_len, d_model=d_model, num_heads=num_heads, dff=dff, num_layers=num_layers, dropout=dropout)\n",
        "    # build with dummy input\n",
        "    dummy = tf.zeros((1, seq_len, n_assets))\n",
        "    _ = model(dummy)\n",
        "    # compile\n",
        "    model.compile(optimizer='adam', loss={'mean_head': 'mse', 'var_head': 'mse'}, metrics={'mean_head':'mae','var_head':'mae'})\n",
        "    env.transformer_model = model\n",
        "\n",
        "    # Optionally load weights safely\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded transformer weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load transformer weights from {model_path}: {e}\\n→ Continuing with freshly initialized transformer.\")\"\"\"\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper\n",
        "# -------------------------\n",
        "\"\"\"def build_transformer_for_env(env, n_assets=18, seq_len=60, node_features=1,\n",
        "                                  d_model=64, num_heads=4, dff=128, num_layers=2,\n",
        "                                  gcn_units=32, dropout=0.1, model_path=None):\n",
        "    model = TransformerModel(n_assets=n_assets, seq_len=seq_len, node_features=node_features,\n",
        "                           d_model=d_model, num_heads=num_heads, dff=dff,\n",
        "                           num_layers=num_layers, gcn_units=gcn_units, dropout=dropout)\n",
        "    dummy_x = tf.zeros((1, seq_len, n_assets, node_features))\n",
        "    _ = model(dummy_x)  # build\n",
        "    model.compile(optimizer=\"adam\", loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "                  metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"})\n",
        "    env.transformer_model = model\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded GCN+Transformer weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load weights from {model_path}: {e}\")\"\"\"\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper for env (keeps naming, weight loading)\n",
        "# -------------------------\n",
        "\"\"\"def build_transformer_for_env(env,\n",
        "                              n_assets=18,\n",
        "                              seq_len=60,\n",
        "                              node_features=1,\n",
        "                              gcn_units=[32, 32],\n",
        "                              lstm_units=64,\n",
        "                              tcn_filters=64,\n",
        "                              tcn_kernel=3,\n",
        "                              tcn_blocks=2,\n",
        "                              model_path=None):\n",
        "    model = TransformerModel(\n",
        "        n_assets=n_assets,\n",
        "        seq_len=seq_len,\n",
        "        node_features=node_features,\n",
        "        gcn_units=gcn_units,\n",
        "        lstm_units=lstm_units,\n",
        "        tcn_filters=tcn_filters,\n",
        "        tcn_kernel=tcn_kernel,\n",
        "        tcn_blocks=tcn_blocks\n",
        "    )\n",
        "\n",
        "    # Build with dummy input\n",
        "    dummy_x = tf.zeros((1, seq_len, n_assets, node_features))\n",
        "    _ = model(dummy_x)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "        metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"}\n",
        "    )\n",
        "    env.transformer_model = model\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded GCN+LSTM+TCN weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load weights from {model_path}: {e}\")\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper for env\n",
        "# -------------------------\n",
        "\"\"\"def build_transformer_for_env(env,\n",
        "                      n_assets=18,\n",
        "                      seq_len=60,\n",
        "                      node_features=1,\n",
        "                      gcn_units=[32, 32],\n",
        "                      model_path=None):\n",
        "    model = TransformerModel(\n",
        "        n_assets=n_assets,\n",
        "        seq_len=seq_len,\n",
        "        node_features=node_features,\n",
        "        gcn_units=gcn_units\n",
        "    )\n",
        "\n",
        "    # Build with dummy input\n",
        "    dummy_x = tf.zeros((1, seq_len, n_assets, node_features))\n",
        "    _ = model(dummy_x)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "        metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"}\n",
        "    )\n",
        "    env.transformer_model = model\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded GNN weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load weights (architecture mismatch). Starting fresh.\")\"\"\"\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper for env\n",
        "# -------------------------\n",
        "\"\"\"def build_transformer_for_env(env,\n",
        "                              n_assets=18,\n",
        "                              seq_len=60,\n",
        "                              tcn_filters=64,\n",
        "                              tcn_kernel=3,\n",
        "                              tcn_blocks=3,\n",
        "                              dropout=0.1,\n",
        "                              model_path=None):\n",
        "    model = TransformerModel(\n",
        "        n_assets=n_assets,\n",
        "        seq_len=seq_len,\n",
        "        tcn_filters=tcn_filters,\n",
        "        tcn_kernel=tcn_kernel,\n",
        "        tcn_blocks=tcn_blocks,\n",
        "        dropout=dropout\n",
        "    )\n",
        "\n",
        "    # Build with dummy input\n",
        "    dummy_x = tf.zeros((1, seq_len, n_assets))\n",
        "    _ = model(dummy_x)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "        metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"}\n",
        "    )\n",
        "    env.transformer_model = model\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded TCN weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load weights (architecture mismatch). Starting fresh.\")\"\"\"\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper for env\n",
        "# -------------------------\n",
        "def build_transformer_for_env(env,\n",
        "                              n_assets=18,\n",
        "                              seq_len=60,\n",
        "                              tcn_filters=64,\n",
        "                              tcn_kernel=3,\n",
        "                              tcn_blocks=3,\n",
        "                              lstm_units=64,\n",
        "                              dropout=0.1,\n",
        "                              model_path=None):\n",
        "    model = TransformerModel(\n",
        "        n_assets=n_assets,\n",
        "        seq_len=seq_len,\n",
        "        tcn_filters=tcn_filters,\n",
        "        tcn_kernel=tcn_kernel,\n",
        "        tcn_blocks=tcn_blocks,\n",
        "        lstm_units=lstm_units,\n",
        "        dropout=dropout\n",
        "    )\n",
        "\n",
        "    # Build with dummy input\n",
        "    dummy_x = tf.zeros((1, seq_len, n_assets))\n",
        "    _ = model(dummy_x)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "        metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"}\n",
        "    )\n",
        "    env.transformer_model = model\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded TCN+LSTM weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load weights (architecture mismatch). Starting fresh.\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper for env\n",
        "# -------------------------\n",
        "\"\"\"def build_transformer_for_env(env,\n",
        "                              n_assets=18,\n",
        "                              seq_len=60,\n",
        "                              d_model=64,\n",
        "                              num_heads=4,\n",
        "                              dff=128,\n",
        "                              num_layers=2,\n",
        "                              dropout=0.1,\n",
        "                              model_path=None):\n",
        "    model = TransformerModel(\n",
        "        n_assets=n_assets,\n",
        "        seq_len=seq_len,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dff=dff,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout\n",
        "    )\n",
        "\n",
        "    dummy = tf.zeros((1, seq_len, n_assets))\n",
        "    _ = model(dummy)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "        metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"}\n",
        "    )\n",
        "    env.transformer_model = model\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded Informer weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load Informer weights from {model_path}: {e}\\n→ Starting fresh.\")\"\"\"\n",
        "\n",
        "# -------------------------\n",
        "# Builder for environment\n",
        "# -------------------------\n",
        "\"\"\"def build_transformer_for_env(env, n_assets=18, seq_len=60, d_model=64, num_heads=4, dff=128, num_layers=2, dropout=0.1, model_path=None):\n",
        "    model = TransformerModel(\n",
        "        n_assets=n_assets,\n",
        "        seq_len=seq_len,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dff=dff,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout\n",
        "    )\n",
        "\n",
        "    # dummy input to build model\n",
        "    dummy_x = tf.zeros((1, seq_len, n_assets))\n",
        "    _ = model(dummy_x)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "        metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"}\n",
        "    )\n",
        "    env.transformer_model = model\n",
        "\n",
        "    # optionally load weights\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded Informer weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load weights. Starting fresh: {e}\")\"\"\"\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Builder wrapper\n",
        "# -------------------------\n",
        "\"\"\"def build_transformer_for_env(env, n_assets=18, seq_len=60,\n",
        "                                    d_model=64, num_heads=4, dff=128,\n",
        "                                    num_layers=2, dropout=0.1, model_path=None):\n",
        "    model = TransformerModel(n_assets=n_assets, seq_len=seq_len,\n",
        "                            d_model=d_model, num_heads=num_heads,\n",
        "                            dff=dff, num_layers=num_layers, dropout=dropout)\n",
        "    # build with dummy input\n",
        "    dummy = tf.zeros((1, seq_len, n_assets))\n",
        "    _ = model(dummy)\n",
        "\n",
        "    # compile\n",
        "    model.compile(optimizer=\"adam\",\n",
        "                  loss={\"mean_head\": \"mse\", \"var_head\": \"mse\"},\n",
        "                  metrics={\"mean_head\": \"mae\", \"var_head\": \"mae\"})\n",
        "    env.transformer_model = model\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"✅ Loaded short-sequence Informer+PE weights from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load weights from {model_path}: {e}\\n→ Starting fresh.\")\"\"\"\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# train_transformer with checkpoint (weights-only)\n",
        "# -------------------------\n",
        "def train_transformer(env, epochs=50, batch_size=16, save_transformer_path=\"best_transformer_online.weights.h5\", validation_split=0.1, verbose=1):\n",
        "    # env.X_train: (N, seq_len, n_assets)\n",
        "    # env.y_train: dict {\"mean_head\": arr, \"var_head\": arr}\n",
        "    if not hasattr(env, \"X_train\") or not hasattr(env, \"y_train\"):\n",
        "        raise ValueError(\"env.X_train and env.y_train must be set before calling train_transformer.\")\n",
        "\n",
        "    if not isinstance(env.y_train, dict) or \"mean_head\" not in env.y_train or \"var_head\" not in env.y_train:\n",
        "        raise ValueError(\"env.y_train must be dict with keys 'mean_head' and 'var_head'.\")\n",
        "\n",
        "    # ensure transformer built\n",
        "    if not hasattr(env, \"transformer_model\") or env.transformer_model is None:\n",
        "        build_transformer_for_env(env, n_assets=env.n_assets, seq_len=env.seq_len, model_path=None)\n",
        "\n",
        "    # ensure compiled\n",
        "    if not hasattr(env.transformer_model, 'optimizer') or env.transformer_model.optimizer is None:\n",
        "        env.transformer_model.compile(optimizer='adam', loss={'mean_head':'mse', 'var_head':'mse'})\n",
        "\n",
        "    # checkpoint callback (weights-only)\n",
        "    checkpoint = ModelCheckpoint(save_transformer_path, monitor=\"val_loss\", save_best_only=True, save_weights_only=True, verbose=1)\n",
        "\n",
        "    history = env.transformer_model.fit(\n",
        "        env.X_train,\n",
        "        [env.y_train['mean_head'], env.y_train['var_head']],\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=validation_split,\n",
        "        callbacks=[checkpoint],\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    print(f\"✅ Transformer training finished. Best weights saved to: {save_transformer_path}\")\n",
        "    return history\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_agent(env,\n",
        "                agent,\n",
        "                max_episodes=10,\n",
        "                batch_size=32,\n",
        "                rebalance_period=5,\n",
        "                n_assets=18,\n",
        "                seq_len=60,\n",
        "                window_size=200,\n",
        "                buffer_size=50_000,\n",
        "                train_every=10,\n",
        "                save_agent_path=\"best_agent.weights.h5\",\n",
        "                save_transformer_path=\"best_transformer_online.weights.h5\"):\n",
        "\n",
        "    # make sure transformer exists before training\n",
        "    if not hasattr(env, \"transformer_model\") or env.transformer_model is None:\n",
        "        build_transformer_for_env(env, n_assets=n_assets, seq_len=seq_len)\n",
        "\n",
        "    # pre-train transformer ONCE\n",
        "    train_transformer(env, epochs=3, batch_size=batch_size,\n",
        "                      save_transformer_path=save_transformer_path)\n",
        "\n",
        "    env.online_train = True\n",
        "\n",
        "    # persistent replay buffer\n",
        "    if not hasattr(agent.memory4replay, \"maxlen\"):\n",
        "        agent.memory4replay = deque(maxlen=buffer_size)\n",
        "\n",
        "    reward_list, returns_per_episode = [], []\n",
        "    early_stop = EarlyStoppingRL(monitor='reward', mode='max',\n",
        "                                 patience=5, min_delta=1e-3, verbose=True)\n",
        "\n",
        "    if not hasattr(agent, \"model\") or agent.model is None:\n",
        "        agent.model = agent._model()\n",
        "\n",
        "    # TRAINING LOOP\n",
        "    for e in range(max_episodes):\n",
        "        agent.is_eval = False\n",
        "        print(f\"\\n📘 Episode {e+1}/{max_episodes} — epsilon={agent.epsilon:.4f}\")\n",
        "\n",
        "        data_length = len(env.data)\n",
        "        returns_history, returns_history_equal, rewards_history = [], [], []\n",
        "\n",
        "        # random start\n",
        "        start_idx = np.random.randint(window_size + 1,\n",
        "                                      max((data_length - 750 - window_size - 1), window_size + 2))\n",
        "        s = env.get_state(start_idx, window_size)\n",
        "\n",
        "        # portfolio init\n",
        "        W0, W0eq = np.zeros(n_assets), np.zeros(n_assets)\n",
        "        capital, capitaleq = 1000.0, 1000.0\n",
        "\n",
        "        step_count = 0\n",
        "        for t in range(window_size, data_length - 750, rebalance_period):\n",
        "            step_count += 1\n",
        "            date1 = t - rebalance_period\n",
        "            s_ = env.get_state(t, window_size)\n",
        "\n",
        "            # agent action\n",
        "            action = agent.act(s_)\n",
        "            w_action = action[0] if isinstance(action, (tuple, list)) else action\n",
        "\n",
        "            # rewards (daily expanded returns)\n",
        "            weighted_returns, reward, W0, capital = env.get_reward(w_action, date1, t, W0, capital, 'rl')\n",
        "            weighted_returns_equal, reward_equal, W0eq, capitaleq = env.get_reward(\n",
        "                np.ones(n_assets)/n_assets, date1, t, W0eq, capitaleq, 'eq'\n",
        "            )\n",
        "\n",
        "            rewards_history.append(float(np.mean(reward)))\n",
        "            returns_history.extend(weighted_returns)           # ✅ daily expand\n",
        "            returns_history_equal.extend(weighted_returns_equal)\n",
        "\n",
        "            done = (t == data_length)\n",
        "            agent.memory4replay.append((s, s_, action, reward, done))\n",
        "\n",
        "            if len(agent.memory4replay) >= batch_size and step_count % train_every == 0:\n",
        "                agent.expReplay(batch_size)\n",
        "\n",
        "            s = s_\n",
        "\n",
        "        # --- episode end\n",
        "        avg_reward = np.mean(rewards_history) if rewards_history else 0.0\n",
        "        reward_list.append(avg_reward)\n",
        "        cum_returns = np.array(returns_history).cumsum()\n",
        "        returns_per_episode.append(cum_returns)\n",
        "\n",
        "        print(f\"Episode {e+1} → avg_reward={avg_reward:.6f}\")\n",
        "\n",
        "        # save models\n",
        "        if avg_reward > early_stop.best_score:\n",
        "            agent.model.save_weights(save_agent_path)\n",
        "            env.transformer_model.save_weights(save_transformer_path)\n",
        "            print(f\"📦 Saved best models (episode {e+1}, reward={avg_reward:.6f})\")\n",
        "\n",
        "        # 🔹 train transformer ONCE per episode\n",
        "        train_transformer(env, epochs=3, save_transformer_path=save_transformer_path)\n",
        "\n",
        "        daily_cum_returns = np.repeat(cum_returns, rebalance_period)\n",
        "        daily_cum_returns = daily_cum_returns[:750]  # truncate to exactly 750 days\n",
        "\n",
        "        daily_cum_returns_equal = np.repeat(np.array(returns_history_equal).cumsum(), rebalance_period)\n",
        "        daily_cum_returns_equal = daily_cum_returns_equal[:750]\n",
        "        # plot last 250 days for visualization\n",
        "        if (e+1) % 5 == 0 or e == max_episodes-1:\n",
        "            plt.figure(figsize=(12, 2))\n",
        "            plt.plot(daily_cum_returns, color=\"black\", label=\"RL agent\")\n",
        "            plt.plot(daily_cum_returns_equal, color=\"grey\", ls=\"--\", label=\"Equal-weighted\")\n",
        "            plt.title(f\"Episode {e+1} Last 750 Days Cumulative Return\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "        if early_stop.check(avg_reward, e):\n",
        "            print(\"✅ Early stopping triggered; ending training loop.\")\n",
        "            break\n",
        "\n",
        "    return reward_list, returns_per_episode\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_agent(env, agent, n_assets=18, seq_len=60, rebalance_period=5, window_size=200,\n",
        "                   load_agent_path=\"best_agent.weights.h5\",\n",
        "                   load_transformer_path=\"best_transformer_online.weights.h5\"):\n",
        "\n",
        "    env.online_train = False\n",
        "    agent.is_eval = True\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    # reload agent\n",
        "    agent.model = agent._model()\n",
        "    if os.path.exists(load_agent_path):\n",
        "        agent.model.load_weights(load_agent_path)\n",
        "        print(f\"✅ Loaded agent weights: {load_agent_path}\")\n",
        "\n",
        "    # rebuild transformer (same architecture as training)\n",
        "    build_transformer_for_env(env, n_assets=n_assets, seq_len=seq_len)\n",
        "\n",
        "    # init weights before load\n",
        "    dummy_input = tf.zeros((1, seq_len, n_assets))\n",
        "    _ = env.transformer_model(dummy_input)\n",
        "\n",
        "    # load saved weights\n",
        "    if os.path.exists(load_transformer_path):\n",
        "        env.transformer_model.load_weights(load_transformer_path)\n",
        "        print(f\"✅ Loaded transformer weights: {load_transformer_path}\")\n",
        "\n",
        "    # --- run evaluation loop (last 250 days)\n",
        "    start_test = len(env.data) - 750\n",
        "    returns_rl, returns_equal = [], []\n",
        "\n",
        "    W0_rl, W0_eq = np.zeros(n_assets), np.zeros(n_assets)\n",
        "    capital_rl, capital_eq = 1000.0, 1000.0\n",
        "    actions_equal, actions_rl = [], []\n",
        "\n",
        "    for t in range(start_test, len(env.data), rebalance_period):\n",
        "        state = env.get_state(t, window_size)\n",
        "        action = agent.act(state)\n",
        "        w_action = action[0] if isinstance(action, (tuple, list)) else action\n",
        "\n",
        "        weighted_returns_rl, reward_rl, W0_rl, capital_rl = env.get_reward(w_action, t-rebalance_period, t, W0_rl, capital_rl, 'rl')\n",
        "        weighted_returns_eq, reward_eq, W0_eq, capital_eq = env.get_reward(np.ones(n_assets)/n_assets, t-rebalance_period, t, W0_eq, capital_eq, 'eq')\n",
        "\n",
        "        # expand daily\n",
        "        returns_rl.extend(weighted_returns_rl)\n",
        "        returns_equal.extend(weighted_returns_eq)\n",
        "\n",
        "        for _ in range(len(weighted_returns_rl)):\n",
        "            actions_equal.append(np.ones(n_assets)/n_assets)\n",
        "            actions_rl.append(w_action)\n",
        "\n",
        "    # truncate to exactly 250 days\n",
        "    returns_rl = returns_rl[:750]\n",
        "    returns_equal = returns_equal[:750]\n",
        "    actions_equal = actions_equal[:750]\n",
        "    actions_rl = actions_rl[:750]\n",
        "\n",
        "    mean_rl = np.mean(returns_rl)\n",
        "    mean_eq = np.mean(returns_equal)\n",
        "\n",
        "    return returns_rl, returns_equal, mean_rl, mean_eq, actions_equal, actions_rl\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jPXYH7QiY8K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Define a helper function for multi-output transformer (mean + variance)\n",
        "def make_transformer_data(df, seq_len=60, n_assets=18):\n",
        "    \"\"\"\n",
        "    Convert env.data (price dataframe) into (X, {y_mean, y_var}) sequences\n",
        "    for multi-output transformer training.\n",
        "    - y_mean = next step returns\n",
        "    - y_var = rolling variance over past seq_len steps\n",
        "    \"\"\"\n",
        "    df = df.astype(float).pct_change().dropna()\n",
        "\n",
        "    X, y_mean, y_var = [], [], []\n",
        "    for i in range(len(df) - seq_len):\n",
        "        x_i = df.iloc[i:i+seq_len].values\n",
        "        y_i = df.iloc[i+seq_len].values      # mean target = next step returns\n",
        "        var_i = np.var(x_i, axis=0)          # variance target = past seq_len var\n",
        "\n",
        "        X.append(x_i)\n",
        "        y_mean.append(y_i)\n",
        "        y_var.append(var_i)\n",
        "\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y_mean = np.array(y_mean, dtype=np.float32)\n",
        "    y_var = np.array(y_var, dtype=np.float32)\n",
        "\n",
        "    # return multi-output labels as dict\n",
        "    return X, {\"mean_head\": y_mean, \"var_head\": y_var}\n",
        "\n",
        "# 2️⃣ Build dataset and attach to env\n",
        "asset_cols = [\n",
        "    '<CLOSE>_BANK','<CLOSE>_BMLT','<CLOSE>_DTIP','<CLOSE>_FOLD','<CLOSE>_GDIR',\n",
        "    '<CLOSE>_MADN','<CLOSE>_MAPN','<CLOSE>_MSMI','<CLOSE>_NORI','<CLOSE>_OIMC',\n",
        "    '<CLOSE>_PASN','<CLOSE>_PKLJ','<CLOSE>_PNES','<CLOSE>_PTAP','<CLOSE>_SFKZ',\n",
        "    '<CLOSE>_SSAP','<CLOSE>_TAMN','<CLOSE>_TSBE'\n",
        "]\n",
        "\n",
        "X_train, y_train = make_transformer_data(env.data[asset_cols], seq_len=60, n_assets=18)\n",
        "\n",
        "# attach to env\n",
        "env.X_train, env.y_train = X_train, y_train\n",
        "\n",
        "# Train\n",
        "rewards = train_agent(env, agent, max_episodes=5, batch_size=32, rebalance_period=5)\n",
        "\n",
        "# Evaluate\n",
        "result_rl, result_equal, mean_rl, mean_eq,actions_equal, actions_rl = evaluate_agent(env, agent)\n",
        "\n"
      ],
      "metadata": {
        "id": "KnffSa2nY8Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Slice data exactly like the RL environment\n",
        "dataind = read_csv('/content/drive/MyDrive/indexnew.csv', index_col=0, header=2)\n",
        "dataind = dataind.rename(columns={'Unnamed: 1': 'IDXS'})\n",
        "dataind = dataind[len(env.data)-750:len(env.data)].reset_index(drop=True)\n",
        "\n",
        "# Prepare rebalance chunks\n",
        "rebalance_period = 5\n",
        "fgh = [dataind['IDXS'][i:i+rebalance_period] for i in range(0, len(dataind), rebalance_period)]\n",
        "\n",
        "# Compute returns per chunk (no NaN drop, first return = 0)\n",
        "x111 = [((chunk - chunk.shift(1)).fillna(0) / chunk.shift(1).replace(0, np.nan)).fillna(0).values\n",
        "        for chunk in fgh]\n",
        "\n",
        "# Concatenate all returns\n",
        "xxx = np.concatenate(x111)\n",
        "\n",
        "# Align length exactly with RL results\n",
        "xxx = xxx[:len(result_rl)]\n",
        "print(len(xxx), len(result_rl))  # Should now match\n"
      ],
      "metadata": {
        "id": "k-wAcDDsZSMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#result_equal_vis = [item for sublist in result_equal for item in sublist]\n",
        "#result_rl_vis = [item for sublist in result_rl for item in sublist]\n",
        "result_equal_vis = result_equal\n",
        "result_rl_vis = result_rl\n"
      ],
      "metadata": {
        "id": "ksrEa10zZSPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10, 5))\n",
        "plt.plot(np.array(result_equal_vis).cumsum(), label = 'Benchmark', color = 'grey',ls = '--')\n",
        "plt.plot(np.array(result_rl_vis).cumsum(), label = 'Deep RL portfolio', color = 'black',ls = '-')\n",
        "plt.plot(np.array(xxx).cumsum(), label = 'Index', color = 'green',ls = '-.')\n",
        "df11133445566=pd.DataFrame(np.array(xxx).cumsum(),columns=['Index']).join(pd.DataFrame(np.array(result_equal_vis).cumsum(),columns=['Benchmark']))\n",
        "df1113344556677=pd.DataFrame(np.array(result_rl_vis).cumsum(),columns=['Deep RL portfolio']).join(df11133445566)\n",
        "for var in list(['Benchmark','Deep RL portfolio','Index']):\n",
        "    plt.annotate('%0.2f' % df1113344556677[var].iloc[-1], xy=(1, df1113344556677[var].iloc[-1]), xytext=(8, 0),\n",
        "                 xycoords=('axes fraction', 'data'), textcoords='offset points',size=10, va=\"center\")\n",
        "plt.xlabel('Date')\n",
        "# Set the y axis label of the current axis.\n",
        "plt.ylabel('Cumluative_Return')\n",
        "# Set a title of the current axes.\n",
        "plt.title('Comparison of Performance')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1keOm0FlZSR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "from statsmodels import regression\n",
        "rf=0.21\n",
        "def sharpe(R):\n",
        "    #r = np.diff(R)\n",
        "    sr = (R.mean()*252-rf)/(R.std() * np.sqrt(252))\n",
        "    return sr\n",
        "\n",
        "def sortino_ratio(series):\n",
        "    #series = np.diff(series)\n",
        "    mean = series.mean() * 252\n",
        "    std_neg = series[series<0].std() * np.sqrt(252)\n",
        "    sor=(mean-rf)/std_neg\n",
        "    return sor\n",
        "\n",
        "def max_drawdown(return_series):\n",
        "    comp_ret = pd.Series((return_series+1).cumprod())\n",
        "    peak = comp_ret.expanding(min_periods=1).max()\n",
        "    dd = (comp_ret/peak)-1\n",
        "    return dd.min()\n",
        "\n",
        "def print_stats(result, benchmark):\n",
        "\n",
        "    sharpe_ratio = sharpe(np.array(result))\n",
        "    sor = sortino_ratio(np.array(result))\n",
        "    max_draw=max_drawdown(np.array(result))\n",
        "    returns = np.mean(np.array(result)*252)\n",
        "    volatility = np.std(np.array(result)*np.sqrt(252))\n",
        "    calmars = (returns-rf)/abs(max_draw)\n",
        "\n",
        "    X = benchmark\n",
        "    y = result\n",
        "    x = sm.add_constant(X)\n",
        "    model = regression.linear_model.OLS(y, x).fit()\n",
        "    alpha = model.params[0]*252\n",
        "    beta = model.params[1]\n",
        "\n",
        "\n",
        "\n",
        "    return np.round(np.array([returns, volatility, sharpe_ratio,sor,max_draw,calmars, alpha, beta]), 4).tolist()"
      ],
      "metadata": {
        "id": "D77YqgLCZSUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(result_equal_vis).to_csv('/content/drive/MyDrive/equal_three.csv')\n",
        "pd.DataFrame(xxx).to_csv('/content/drive/MyDrive/index_three.csv')\n",
        "pd.DataFrame(result_rl_vis).to_csv('/content/drive/MyDrive/tcn+lstm_BL_TRA_three.csv')"
      ],
      "metadata": {
        "id": "YtYEbRzTZSXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rl_infcnn1= read_csv('/content/drive/MyDrive/informer_BL_cnn_one.csv',index_col=0)\n",
        "rl_infcnn1.columns = ['Informer_BL_Cnn_One']\n",
        "rl_infcnn2= read_csv('/content/drive/MyDrive/informer_BL_cnn_two.csv',index_col=0)\n",
        "rl_infcnn2.columns = ['Informer_BL_Cnn_Two']\n",
        "rl_infcnn3= read_csv('/content/drive/MyDrive/informer_BL_cnn_three.csv',index_col=0)\n",
        "rl_infcnn3.columns = ['Informer_BL_Cnn_Three']\n",
        "rl_inftra1= read_csv('/content/drive/MyDrive/informer_BL_TRA_one.csv',index_col=0)\n",
        "rl_inftra1.columns = ['Informer_BL_Tra_One']\n",
        "rl_inftra2= read_csv('/content/drive/MyDrive/informer_BL_TRA_two.csv',index_col=0)\n",
        "rl_inftra2.columns = ['Informer_BL_Tra_Two']\n",
        "rl_inftra3= read_csv('/content/drive/MyDrive/informer_BL_TRA_three.csv',index_col=0)\n",
        "rl_inftra3.columns = ['Informer_BL_Tra_Three']\n",
        "\n",
        "rl_tracnn1= read_csv('/content/drive/MyDrive/transformer_BL_cnn_one.csv',index_col=0)\n",
        "rl_tracnn1.columns = ['Transformer_BL_Cnn_One']\n",
        "rl_tracnn2= read_csv('/content/drive/MyDrive/transformer_BL_cnn_two.csv',index_col=0)\n",
        "rl_tracnn2.columns = ['Transformer_BL_Cnn_Two']\n",
        "rl_tracnn3= read_csv('/content/drive/MyDrive/transformer_BL_cnn_three.csv',index_col=0)\n",
        "rl_tracnn3.columns = ['Transformer_BL_Cnn_Three']\n",
        "rl_tratra1= read_csv('/content/drive/MyDrive/transformer_BL_TRA_one.csv',index_col=0)\n",
        "rl_tratra1.columns = ['Transformer_BL_Tra_One']\n",
        "rl_tratra2= read_csv('/content/drive/MyDrive/transformer_BL_TRA_two.csv',index_col=0)\n",
        "rl_tratra2.columns = ['Transformer_BL_Tra_Two']\n",
        "rl_tratra3= read_csv('/content/drive/MyDrive/transformer_BL_TRA_three.csv',index_col=0)\n",
        "rl_tratra3.columns = ['Transformer_BL_Tra_Three']\n",
        "\n",
        "rl_tragcncnn1= read_csv('/content/drive/MyDrive/transformer+gcn_BL_cnn_one.csv',index_col=0)\n",
        "rl_tragcncnn1.columns = ['Transformer+Gcn_BL_Cnn_One']\n",
        "rl_tragcncnn2= read_csv('/content/drive/MyDrive/transformer+gcn_BL_cnn_two.csv',index_col=0)\n",
        "rl_tragcncnn2.columns = ['Transformer+Gcn_BL_Cnn_Two']\n",
        "rl_tragcncnn3= read_csv('/content/drive/MyDrive/transformer+gcn_BL_cnn_three.csv',index_col=0)\n",
        "rl_tragcncnn3.columns = ['Transformer+Gcn_BL_Cnn_Three']\n",
        "rl_tragcntra1= read_csv('/content/drive/MyDrive/transformer+gcn_BL_TRA_one.csv',index_col=0)\n",
        "rl_tragcntra1.columns = ['Transformer+Gcn_BL_Tra_One']\n",
        "rl_tragcntra2= read_csv('/content/drive/MyDrive/transformer+gcn_BL_TRA_two.csv',index_col=0)\n",
        "rl_tragcntra2.columns = ['Transformer+Gcn_BL_Tra_Two']\n",
        "rl_tragcntra3= read_csv('/content/drive/MyDrive/transformer+gcn_BL_TRA_three.csv',index_col=0)\n",
        "rl_tragcntra3.columns = ['Transformer+Gcn_BL_Tra_Three']\n",
        "\n",
        "rl_gcnlstmtcncnn1= read_csv('/content/drive/MyDrive/gcn+lstm+tcn_BL_cnn_one.csv',index_col=0)\n",
        "rl_gcnlstmtcncnn1.columns = ['Gcn+Lstm+Tcn_BL_Cnn_One']\n",
        "rl_gcnlstmtcncnn2= read_csv('/content/drive/MyDrive/gcn+lstm+tcn_BL_cnn_two.csv',index_col=0)\n",
        "rl_gcnlstmtcncnn2.columns = ['Gcn+Lstm+Tcn_BL_Cnn_Two']\n",
        "rl_gcnlstmtcncnn3= read_csv('/content/drive/MyDrive/gcn+lstm+tcn_BL_cnn_three.csv',index_col=0)\n",
        "rl_gcnlstmtcncnn3.columns = ['Gcn+Lstm+Tcn_BL_Cnn_Three']\n",
        "rl_gcnlstmtcntra1= read_csv('/content/drive/MyDrive/gcn+lstm+tcn_BL_TRA_one.csv',index_col=0)\n",
        "rl_gcnlstmtcntra1.columns = ['Gcn+Lstm+Tcn_BL_Tra_One']\n",
        "rl_gcnlstmtcntra2= read_csv('/content/drive/MyDrive/gcn+lstm+tcn_BL_TRA_two.csv',index_col=0)\n",
        "rl_gcnlstmtcntra2.columns = ['Gcn+Lstm+Tcn_BL_Tra_Two']\n",
        "rl_gcnlstmtcntra3= read_csv('/content/drive/MyDrive/gcn+lstm+tcn_BL_TRA_three.csv',index_col=0)\n",
        "rl_gcnlstmtcntra3.columns = ['Gcn+Lstm+Tcn_BL_Tra_Three']\n",
        "\n",
        "rl_gnncnn1= read_csv('/content/drive/MyDrive/gnn_BL_cnn_one.csv',index_col=0)\n",
        "rl_gnncnn1.columns = ['Gnn_BL_Cnn_One']\n",
        "rl_gnncnn2= read_csv('/content/drive/MyDrive/gnn_BL_cnn_two.csv',index_col=0)\n",
        "rl_gnncnn2.columns = ['Gnn_BL_Cnn_Two']\n",
        "rl_gnncnn3= read_csv('/content/drive/MyDrive/gnn_BL_cnn_three.csv',index_col=0)\n",
        "rl_gnncnn3.columns = ['Gnn_BL_Cnn_Three']\n",
        "rl_gnntra1= read_csv('/content/drive/MyDrive/gnn_BL_TRA_one.csv',index_col=0)\n",
        "rl_gnntra1.columns = ['Gnn_BL_Tra_One']\n",
        "rl_gnntra2= read_csv('/content/drive/MyDrive/gnn_BL_TRA_two.csv',index_col=0)\n",
        "rl_gnntra2.columns = ['Gnn_BL_Tra_Two']\n",
        "rl_gnntra3= read_csv('/content/drive/MyDrive/gnn_BL_TRA_three.csv',index_col=0)\n",
        "rl_gnntra3.columns = ['Gnn_BL_Tra_Three']\n",
        "\n",
        "rl_tcncnn1= read_csv('/content/drive/MyDrive/tcn_BL_cnn_one.csv',index_col=0)\n",
        "rl_tcncnn1.columns = ['Tcn_BL_Cnn_One']\n",
        "rl_tcncnn2= read_csv('/content/drive/MyDrive/tcn_BL_cnn_two.csv',index_col=0)\n",
        "rl_tcncnn2.columns = ['Tcn_BL_Cnn_Two']\n",
        "rl_tcncnn3= read_csv('/content/drive/MyDrive/tcn_BL_cnn_three.csv',index_col=0)\n",
        "rl_tcncnn3.columns = ['Tcn_BL_Cnn_Three']\n",
        "rl_tcntra1= read_csv('/content/drive/MyDrive/tcn_BL_TRA_one.csv',index_col=0)\n",
        "rl_tcntra1.columns = ['Tcn_BL_Tra_One']\n",
        "rl_tcntra2= read_csv('/content/drive/MyDrive/tcn_BL_TRA_two.csv',index_col=0)\n",
        "rl_tcntra2.columns = ['Tcn_BL_Tra_Two']\n",
        "rl_tcntra3= read_csv('/content/drive/MyDrive/tcn_BL_TRA_three.csv',index_col=0)\n",
        "rl_tcntra3.columns = ['Tcn_BL_Tra_Three']\n",
        "\n",
        "\n",
        "rl_tcnlstmcnn1= read_csv('/content/drive/MyDrive/tcn+lstm_BL_cnn_one.csv',index_col=0)\n",
        "rl_tcnlstmcnn1.columns = ['Tcn+Lstm_BL_Cnn_One']\n",
        "rl_tcnlstmcnn2= read_csv('/content/drive/MyDrive/tcn+lstm_BL_cnn_two.csv',index_col=0)\n",
        "rl_tcnlstmcnn2.columns = ['Tcn+Lstm_BL_Cnn_Two']\n",
        "rl_tcnlstmcnn3= read_csv('/content/drive/MyDrive/tcn+lstm_BL_cnn_three.csv',index_col=0)\n",
        "rl_tcnlstmcnn3.columns = ['Tcn+Lstm_BL_Cnn_Three']\n",
        "rl_tcnlstmtra1= read_csv('/content/drive/MyDrive/tcn+lstm_BL_TRA_one.csv',index_col=0)\n",
        "rl_tcnlstmtra1.columns = ['Tcn+Lstm_BL_Tra_One']\n",
        "rl_tcnlstmtra2= read_csv('/content/drive/MyDrive/tcn+lstm_BL_TRA_two.csv',index_col=0)\n",
        "rl_tcnlstmtra2.columns = ['Tcn+Lstm_BL_Tra_Two']\n",
        "rl_tcnlstmtra3= read_csv('/content/drive/MyDrive/tcn+lstm_BL_TRA_three.csv',index_col=0)\n",
        "rl_tcnlstmtra3.columns = ['Tcn+Lstm_BL_Tra_Three']\n",
        "\n",
        "\n",
        "rl_index1= read_csv('/content/drive/MyDrive/index_one.csv',index_col=0)\n",
        "rl_index1.columns = ['Index_One']\n",
        "rl_equal1= read_csv('/content/drive/MyDrive/equal_one.csv',index_col=0)\n",
        "rl_equal1.columns = ['Equal_One']\n",
        "\n",
        "rl_index2= read_csv('/content/drive/MyDrive/index_two.csv',index_col=0)\n",
        "rl_index2.columns = ['Index_Two']\n",
        "rl_equal2= read_csv('/content/drive/MyDrive/equal_two.csv',index_col=0)\n",
        "rl_equal2.columns = ['Equal_Two']\n",
        "\n",
        "rl_index3= read_csv('/content/drive/MyDrive/index_three.csv',index_col=0)\n",
        "rl_index3.columns = ['Index_Three']\n",
        "rl_equal3= read_csv('/content/drive/MyDrive/equal_three.csv',index_col=0)\n",
        "rl_equal3.columns = ['Equal_Three']"
      ],
      "metadata": {
        "id": "1aNhaH0UZSZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vnp=pd.concat([rl_infcnn1.cumsum(),rl_tracnn1.cumsum(),rl_tragcncnn1.cumsum(),rl_gcnlstmtcncnn1.cumsum(),\n",
        "               rl_gnncnn1.cumsum(),rl_tcncnn1.cumsum(),rl_tcnlstmcnn1.cumsum(),rl_index1.cumsum(),rl_equal1.cumsum()],axis=1)\n",
        "plt.figure(figsize = (10, 5))\n",
        "plt.plot(rl_infcnn1.cumsum(), label = 'Informer_BL_Cnn_One',ls = '--')\n",
        "plt.plot(rl_tracnn1.cumsum(), label = 'Transformer_BL_Cnn_One',ls = '-')\n",
        "plt.plot(rl_tragcncnn1.cumsum(), label = 'Transformer+Gcn_BL_Cnn_One',ls = '-.')\n",
        "plt.plot(rl_gcnlstmtcncnn1.cumsum(), label = 'Gcn+Lstm+Tcn_BL_Cnn_One',ls = 'dashdot')\n",
        "plt.plot(rl_gnncnn1.cumsum(), label = 'Gnn_BL_Cnn_One',ls = ':')\n",
        "plt.plot(rl_tcncnn1.cumsum(), label = 'Tcn_BL_Cnn_One',ls = 'solid')\n",
        "plt.plot(rl_tcnlstmcnn1.cumsum(), label = 'Tcn+Lstm_BL_Cnn_One',ls = 'dashed')\n",
        "plt.plot(rl_index1.cumsum(), label = 'Index_One',ls = 'dotted')\n",
        "plt.plot(rl_equal1.cumsum(), label = 'Equal_One')\n",
        "for var in list(vnp.columns):\n",
        "    plt.annotate('%0.2f' % vnp[var].iloc[-1], xy=(1, vnp[var].iloc[-1]), xytext=(8, 0),\n",
        "                 xycoords=('axes fraction', 'data'), textcoords='offset points',size=10, va=\"center\")\n",
        "plt.xlabel('Date')\n",
        "# Set the y axis label of the current axis.\n",
        "plt.ylabel('Cumluative_Return')\n",
        "# Set a title of the current axes.\n",
        "plt.title('Comparison of Performance')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fLbN1jB5ZScy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Informer_BL_Cnn_One', print_stats(rl_infcnn1, rl_index1))\n",
        "print('Transformer_BL_Cnn_One', print_stats(rl_tracnn1, rl_index1))\n",
        "print('Transformer+Gcn_BL_Cnn_One', print_stats(rl_tragcncnn1, rl_index1))\n",
        "print('Gcn+Lstm+Tcn_BL_Cnn_One', print_stats(rl_gcnlstmtcncnn1, rl_index1))\n",
        "print('Gnn_BL_Cnn_One', print_stats(rl_gnncnn1, rl_index1))\n",
        "print('Tcn_BL_Cnn_One', print_stats(rl_tcncnn1, rl_index1))\n",
        "print('Tcn+Lstm_BL_Cnn_One', print_stats(rl_tcnlstmcnn1, rl_index1))\n",
        "print('Equal_One', print_stats(rl_equal1, rl_index1))\n",
        "print('Index_One', print_stats(rl_index1, rl_index1))"
      ],
      "metadata": {
        "id": "rL3cqmwtZoDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library\n",
        "import scipy.stats as stats\n",
        "\n",
        "# pre holds the mileage before\n",
        "# applying the different engine oil\n",
        "pre = rl_tracnn1\n",
        "# post holds the mileage after\n",
        "# applying the different engine oil\n",
        "post = rl_tcncnn1\n",
        "# Performing the paired sample t-test\n",
        "stats.ttest_rel(pre, post,alternative='less')"
      ],
      "metadata": {
        "id": "pOk2rK7HZoG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vnp=pd.concat([rl_infcnn2.cumsum(),rl_tracnn2.cumsum(),rl_tragcncnn2.cumsum(),rl_gcnlstmtcncnn2.cumsum(),\n",
        "               rl_gnncnn2.cumsum(),rl_tcncnn2.cumsum(),rl_tcnlstmcnn2.cumsum(),rl_index2.cumsum(),rl_equal2.cumsum()],axis=1)\n",
        "plt.figure(figsize = (10, 5))\n",
        "plt.plot(rl_infcnn2.cumsum(), label = 'Informer_BL_Cnn_Two',ls = '--')\n",
        "plt.plot(rl_tracnn2.cumsum(), label = 'Transformer_BL_Cnn_Two',ls = '-')\n",
        "plt.plot(rl_tragcncnn2.cumsum(), label = 'Transformer+Gcn_BL_Cnn_Two',ls = '-.')\n",
        "plt.plot(rl_gcnlstmtcncnn2.cumsum(), label = 'Gcn+Lstm+Tcn_BL_Cnn_Two',ls = 'dashdot')\n",
        "plt.plot(rl_gnncnn2.cumsum(), label = 'Gnn_BL_Cnn_Two',ls = ':')\n",
        "plt.plot(rl_tcncnn2.cumsum(), label = 'Tcn_BL_Cnn_Two',ls = 'solid')\n",
        "plt.plot(rl_tcnlstmcnn2.cumsum(), label = 'Tcn+Lstm_BL_Cnn_Two',ls = 'dashed')\n",
        "plt.plot(rl_index2.cumsum(), label = 'Index_Two',ls = 'dotted')\n",
        "plt.plot(rl_equal2.cumsum(), label = 'Equal_Two')\n",
        "for var in list(vnp.columns):\n",
        "    plt.annotate('%0.2f' % vnp[var].iloc[-1], xy=(1, vnp[var].iloc[-1]), xytext=(8, 0),\n",
        "                 xycoords=('axes fraction', 'data'), textcoords='offset points',size=10, va=\"center\")\n",
        "plt.xlabel('Date')\n",
        "# Set the y axis label of the current axis.\n",
        "plt.ylabel('Cumluative_Return')\n",
        "# Set a title of the current axes.\n",
        "plt.title('Comparison of Performance')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uwcYmOy7ZoJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Informer_BL_Cnn_Two', print_stats(rl_infcnn2, rl_index2))\n",
        "print('Transformer_BL_Cnn_Two', print_stats(rl_tracnn2, rl_index2))\n",
        "print('Transformer+Gcn_BL_Cnn_Two', print_stats(rl_tragcncnn2, rl_index2))\n",
        "print('Gcn+Lstm+Tcn_BL_Cnn_Two', print_stats(rl_gcnlstmtcncnn2, rl_index2))\n",
        "print('Gnn_BL_Cnn_Two', print_stats(rl_gnncnn2, rl_index2))\n",
        "print('Tcn_BL_Cnn_Two', print_stats(rl_tcncnn2, rl_index2))\n",
        "print('Tcn+Lstm_BL_Cnn_Two', print_stats(rl_tcnlstmcnn2, rl_index2))\n",
        "print('Equal_Two', print_stats(rl_equal2, rl_index2))\n",
        "print('Index_Two', print_stats(rl_index2, rl_index2))"
      ],
      "metadata": {
        "id": "qy9NNEKuZoMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library\n",
        "import scipy.stats as stats\n",
        "\n",
        "# pre holds the mileage before\n",
        "# applying the different engine oil\n",
        "pre = rl_tracnn2\n",
        "# post holds the mileage after\n",
        "# applying the different engine oil\n",
        "post = rl_tcncnn2\n",
        "# Performing the paired sample t-test\n",
        "stats.ttest_rel(pre, post,alternative='less')"
      ],
      "metadata": {
        "id": "qXJ3UJZBZoPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vnp=pd.concat([rl_infcnn3.cumsum(),rl_tracnn3.cumsum(),rl_tragcncnn3.cumsum(),rl_gcnlstmtcncnn3.cumsum(),\n",
        "               rl_gnncnn3.cumsum(),rl_tcncnn3.cumsum(),rl_tcnlstmcnn3.cumsum(),rl_index3.cumsum(),rl_equal3.cumsum()],axis=1)\n",
        "plt.figure(figsize = (10, 5))\n",
        "plt.plot(rl_infcnn3.cumsum(), label = 'Informer_BL_Cnn_Three',ls = '--')\n",
        "plt.plot(rl_tracnn3.cumsum(), label = 'Transformer_BL_Cnn_Three',ls = '-')\n",
        "plt.plot(rl_tragcncnn3.cumsum(), label = 'Transformer+Gcn_BL_Cnn_Three',ls = '-.')\n",
        "plt.plot(rl_gcnlstmtcncnn3.cumsum(), label = 'Gcn+Lstm+Tcn_BL_Cnn_Three',ls = 'dashdot')\n",
        "plt.plot(rl_gnncnn3.cumsum(), label = 'Gnn_BL_Cnn_Three',ls = ':')\n",
        "plt.plot(rl_tcncnn3.cumsum(), label = 'Tcn_BL_Cnn_Three',ls = 'solid')\n",
        "plt.plot(rl_tcnlstmcnn3.cumsum(), label = 'Tcn+Lstm_BL_Cnn_Three',ls = 'dashed')\n",
        "plt.plot(rl_index3.cumsum(), label = 'Index_Three',ls = 'dotted')\n",
        "plt.plot(rl_equal3.cumsum(), label = 'Equal_Three')\n",
        "for var in list(vnp.columns):\n",
        "    plt.annotate('%0.2f' % vnp[var].iloc[-1], xy=(1, vnp[var].iloc[-1]), xytext=(8, 0),\n",
        "                 xycoords=('axes fraction', 'data'), textcoords='offset points',size=10, va=\"center\")\n",
        "plt.xlabel('Date')\n",
        "# Set the y axis label of the current axis.\n",
        "plt.ylabel('Cumluative_Return')\n",
        "# Set a title of the current axes.\n",
        "plt.title('Comparison of Performance')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DbPy5qJTZoSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Informer_BL_Cnn_Three', print_stats(rl_infcnn3, rl_index3))\n",
        "print('Transformer_BL_Cnn_Three', print_stats(rl_tracnn3, rl_index3))\n",
        "print('Transformer+Gcn_BL_Cnn_Three', print_stats(rl_tragcncnn3, rl_index3))\n",
        "print('Gcn+Lstm+Tcn_BL_Cnn_Three', print_stats(rl_gcnlstmtcncnn3, rl_index3))\n",
        "print('Gnn_BL_Cnn_Three', print_stats(rl_gnncnn3, rl_index3))\n",
        "print('Tcn_BL_Cnn_Three', print_stats(rl_tcncnn3, rl_index3))\n",
        "print('Tcn+Lstm_BL_Cnn_Three', print_stats(rl_tcnlstmcnn3, rl_index3))\n",
        "print('Equal_Three', print_stats(rl_equal3, rl_index3))\n",
        "print('Index_Three', print_stats(rl_index3, rl_index3))"
      ],
      "metadata": {
        "id": "YXkwax09ZoVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library\n",
        "import scipy.stats as stats\n",
        "\n",
        "# pre holds the mileage before\n",
        "# applying the different engine oil\n",
        "pre = rl_tcncnn3\n",
        "# post holds the mileage after\n",
        "# applying the different engine oil\n",
        "post = rl_tracnn3\n",
        "# Performing the paired sample t-test\n",
        "stats.ttest_rel(pre, post,alternative='less')"
      ],
      "metadata": {
        "id": "tIlgXep-ZoXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vnp=pd.concat([rl_inftra1.cumsum(),rl_tratra1.cumsum(),rl_tragcntra1.cumsum(),rl_gcnlstmtcntra1.cumsum(),\n",
        "               rl_gnntra1.cumsum(),rl_tcntra1.cumsum(),rl_tcnlstmtra1.cumsum(),rl_index1.cumsum(),rl_equal1.cumsum()],axis=1)\n",
        "plt.figure(figsize = (10, 5))\n",
        "plt.plot(rl_inftra1.cumsum(), label = 'Informer_BL_Tra_One',ls = '--')\n",
        "plt.plot(rl_tratra1.cumsum(), label = 'Transformer_BL_Tra_One',ls = '-')\n",
        "plt.plot(rl_tragcntra1.cumsum(), label = 'Transformer+Gcn_BL_Tra_One',ls = '-.')\n",
        "plt.plot(rl_gcnlstmtcntra1.cumsum(), label = 'Gcn+Lstm+Tcn_BL_Tra_One',ls = 'dashdot')\n",
        "plt.plot(rl_gnntra1.cumsum(), label = 'Gnn_BL_Tra_One',ls = ':')\n",
        "plt.plot(rl_tcntra1.cumsum(), label = 'Tcn_BL_Tra_One',ls = 'solid')\n",
        "plt.plot(rl_tcnlstmtra1.cumsum(), label = 'Tcn+Lstm_BL_Tra_One',ls = 'dashed')\n",
        "plt.plot(rl_index1.cumsum(), label = 'Index_One',ls = 'dotted')\n",
        "plt.plot(rl_equal1.cumsum(), label = 'Equal_One_Three',)\n",
        "for var in list(vnp.columns):\n",
        "    plt.annotate('%0.2f' % vnp[var].iloc[-1], xy=(1, vnp[var].iloc[-1]), xytext=(8, 0),\n",
        "                 xycoords=('axes fraction', 'data'), textcoords='offset points',size=10, va=\"center\")\n",
        "plt.xlabel('Date')\n",
        "# Set the y axis label of the current axis.\n",
        "plt.ylabel('Cumluative_Return')\n",
        "# Set a title of the current axes.\n",
        "plt.title('Comparison of Performance')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kE8000mHZoaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Informer_BL_Tra_One', print_stats(rl_inftra1, rl_index1))\n",
        "print('Transformer_BL_Tra_One', print_stats(rl_tratra1, rl_index1))\n",
        "print('Transformer+Gcn_BL_Tra_One', print_stats(rl_tragcntra1, rl_index1))\n",
        "print('Gcn+Lstm+Tcn_BL_Tra_One', print_stats(rl_gcnlstmtcntra1, rl_index1))\n",
        "print('Gnn_BL_Tra_One', print_stats(rl_gnntra1, rl_index1))\n",
        "print('Tcn_BL_Tra_One', print_stats(rl_tcntra1, rl_index1))\n",
        "print('Tcn+Lstm_BL_Tra_One', print_stats(rl_tcnlstmtra1, rl_index1))\n",
        "print('Equal_One', print_stats(rl_equal1, rl_index1))\n",
        "print('Index_One', print_stats(rl_index1, rl_index1))"
      ],
      "metadata": {
        "id": "uZLZJvuxZodG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library\n",
        "import scipy.stats as stats\n",
        "\n",
        "# pre holds the mileage before\n",
        "# applying the different engine oil\n",
        "pre = rl_tcnlstmtra1\n",
        "# post holds the mileage after\n",
        "# applying the different engine oil\n",
        "post = rl_tcntra1\n",
        "# Performing the paired sample t-test\n",
        "stats.ttest_rel(pre, post,alternative='less')"
      ],
      "metadata": {
        "id": "C78IdRulYXGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vnp=pd.concat([rl_inftra2.cumsum(),rl_tratra2.cumsum(),rl_tragcntra2.cumsum(),rl_gcnlstmtcntra2.cumsum(),\n",
        "               rl_gnntra2.cumsum(),rl_tcntra2.cumsum(),rl_tcnlstmtra2.cumsum(),rl_index2.cumsum(),rl_equal2.cumsum()],axis=1)\n",
        "plt.figure(figsize = (10, 5))\n",
        "plt.plot(rl_inftra2.cumsum(), label = 'Informer_BL_Tra_Two',ls = '--')\n",
        "plt.plot(rl_tratra2.cumsum(), label = 'Transformer_BL_Tra_Two',ls = '-')\n",
        "plt.plot(rl_tragcntra2.cumsum(), label = 'Transformer+Gcn_BL_Tra_Two',ls = '-.')\n",
        "plt.plot(rl_gcnlstmtcntra2.cumsum(), label = 'Gcn+Lstm+Tcn_BL_Tra_Two',ls = 'dashdot')\n",
        "plt.plot(rl_gnntra2.cumsum(), label = 'Gnn_BL_Tra_Two',ls = ':')\n",
        "plt.plot(rl_tcntra2.cumsum(), label = 'Tcn_BL_Tra_Two',ls = 'solid')\n",
        "plt.plot(rl_tcnlstmtra2.cumsum(), label = 'Tcn+Lstm_BL_Tra_Two',ls = 'dashed')\n",
        "plt.plot(rl_index2.cumsum(), label = 'Index_Two',ls = 'dotted')\n",
        "plt.plot(rl_equal2.cumsum(), label = 'Equal_Two_Three',)\n",
        "for var in list(vnp.columns):\n",
        "    plt.annotate('%0.2f' % vnp[var].iloc[-1], xy=(1, vnp[var].iloc[-1]), xytext=(8, 0),\n",
        "                 xycoords=('axes fraction', 'data'), textcoords='offset points',size=10, va=\"center\")\n",
        "plt.xlabel('Date')\n",
        "# Set the y axis label of the current axis.\n",
        "plt.ylabel('Cumluative_Return')\n",
        "# Set a title of the current axes.\n",
        "plt.title('Comparison of Performance')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WifOKSyZZ8-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Informer_BL_Tra_Two', print_stats(rl_inftra2, rl_index2))\n",
        "print('Transformer_BL_Tra_Two', print_stats(rl_tratra2, rl_index2))\n",
        "print('Transformer+Gcn_BL_Tra_Two', print_stats(rl_tragcntra2, rl_index2))\n",
        "print('Gcn+Lstm+Tcn_BL_Tra_Two', print_stats(rl_gcnlstmtcntra2, rl_index2))\n",
        "print('Gnn_BL_Tra_Two', print_stats(rl_gnntra2, rl_index2))\n",
        "print('Tcn_BL_Tra_Two', print_stats(rl_tcntra2, rl_index2))\n",
        "print('Tcn+Lstm_BL_Tra_Two', print_stats(rl_tcnlstmtra2, rl_index2))\n",
        "print('Equal_Two', print_stats(rl_equal2, rl_index2))\n",
        "print('Index_Two', print_stats(rl_index2, rl_index2))"
      ],
      "metadata": {
        "id": "MolPHCUwZ9Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library\n",
        "import scipy.stats as stats\n",
        "\n",
        "# pre holds the mileage before\n",
        "# applying the different engine oil\n",
        "pre = rl_gnntra2\n",
        "# post holds the mileage after\n",
        "# applying the different engine oil\n",
        "post = rl_tcnlstmtra2\n",
        "# Performing the paired sample t-test\n",
        "stats.ttest_rel(pre, post,alternative='less')"
      ],
      "metadata": {
        "id": "fq5FhG3taAWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vnp=pd.concat([rl_inftra3.cumsum(),rl_tratra3.cumsum(),rl_tragcntra3.cumsum(),rl_gcnlstmtcntra3.cumsum(),\n",
        "               rl_gnntra3.cumsum(),rl_tcntra3.cumsum(),rl_tcnlstmtra3.cumsum(),rl_index3.cumsum(),rl_equal3.cumsum()],axis=1)\n",
        "plt.figure(figsize = (10, 5))\n",
        "plt.plot(rl_inftra3.cumsum(), label = 'Informer_BL_Tra_Three',ls = '--')\n",
        "plt.plot(rl_tratra3.cumsum(), label = 'Transformer_BL_Tra_Three',ls = '-')\n",
        "plt.plot(rl_tragcntra3.cumsum(), label = 'Transformer+Gcn_BL_Tra_Three',ls = '-.')\n",
        "plt.plot(rl_gcnlstmtcntra3.cumsum(), label = 'Gcn+Lstm+Tcn_BL_Tra_Three',ls = 'dashdot')\n",
        "plt.plot(rl_gnntra3.cumsum(), label = 'Gnn_BL_Tra_Three',ls = ':')\n",
        "plt.plot(rl_tcntra3.cumsum(), label = 'Tcn_BL_Tra_Three',ls = 'solid')\n",
        "plt.plot(rl_tcnlstmtra3.cumsum(), label = 'Tcn+Lstm_BL_Tra_Three',ls = 'dashed')\n",
        "plt.plot(rl_index3.cumsum(), label = 'Index_Three',ls = 'dotted')\n",
        "plt.plot(rl_equal3.cumsum(), label = 'Equal_Tra_Three',)\n",
        "for var in list(vnp.columns):\n",
        "    plt.annotate('%0.2f' % vnp[var].iloc[-1], xy=(1, vnp[var].iloc[-1]), xytext=(8, 0),\n",
        "                 xycoords=('axes fraction', 'data'), textcoords='offset points',size=10, va=\"center\")\n",
        "plt.xlabel('Date')\n",
        "# Set the y axis label of the current axis.\n",
        "plt.ylabel('Cumluative_Return')\n",
        "# Set a title of the current axes.\n",
        "plt.title('Comparison of Performance')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wJIvRCxmaCOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Informer_BL_Tra_Three', print_stats(rl_inftra3, rl_index3))\n",
        "print('Transformer_BL_Tra_Three', print_stats(rl_tratra3, rl_index3))\n",
        "print('Transformer+Gcn_BL_Tra_Three', print_stats(rl_tragcntra3, rl_index3))\n",
        "print('Gcn+Lstm+Tcn_BL_Tra_Three', print_stats(rl_gcnlstmtcntra3, rl_index3))\n",
        "print('Gnn_BL_Tra_Three', print_stats(rl_gnntra3, rl_index3))\n",
        "print('Tcn_BL_Tra_Three', print_stats(rl_tcntra3, rl_index3))\n",
        "print('Tcn+Lstm_BL_Tra_Three', print_stats(rl_tcnlstmtra3, rl_index3))\n",
        "print('Equal_Three', print_stats(rl_equal3, rl_index3))\n",
        "print('Index_Three', print_stats(rl_index3, rl_index3))"
      ],
      "metadata": {
        "id": "flAOK2gVaCRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library\n",
        "import scipy.stats as stats\n",
        "\n",
        "# pre holds the mileage before\n",
        "# applying the different engine oil\n",
        "pre = rl_tcntra3\n",
        "# post holds the mileage after\n",
        "# applying the different engine oil\n",
        "post = rl_tcnlstmtra3\n",
        "# Performing the paired sample t-test\n",
        "stats.ttest_rel(pre, post,alternative='less')"
      ],
      "metadata": {
        "id": "7fJbRohNaCUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "companies = ['Fars','Femeli','Foolad','Vabemelat','Nouri','Shasta','Parsan','VaGhadir',\n",
        "             'Tapico','Shepna','VaOmid','Vamaaden','Remapna','Vabank','Sefars','Tipico','Vasapa','Vabshahr']\n",
        "returns = [0.410,0.447,0.427,0.465,0.838,0.556,0.523,0.446,0.399,0.490,0.446,0.448,0.419,0.473,0.371,0.419,0.367,0.332]\n",
        "risks = [0.279,0.298,0.307,0.319,0.380,0.362,0.324,0.297,0.322,0.362,0.235,0.321,0.357,0.288,0.326,0.270,0.434,0.302]\n",
        "sharpe = [0.715,0.797,0.708,0.800,1.652,0.958,0.966,0.792,0.587,0.774,1.003,0.742,0.585,0.913,0.495,0.759,0.363,0.406]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12,8))\n",
        "scatter = plt.scatter(risks, returns, c=sharpe, cmap='viridis', s=[s*200 for s in sharpe], alpha=0.8)\n",
        "for i, company in enumerate(companies):\n",
        "    plt.text(risks[i]+0.005, returns[i]+0.005, company, fontsize=9)\n",
        "plt.xlabel('Annual Risk (Volatility)')\n",
        "plt.ylabel('Average Annual Return')\n",
        "plt.title('Risk vs Return of 18 Selected Iranian Companies')\n",
        "cbar = plt.colorbar(scatter)\n",
        "cbar.set_label('Sharpe Ratio')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "V6yS6V9saCWr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}